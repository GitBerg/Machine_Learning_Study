{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_Redes_Neurais_Artificiais.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfO+gbn36hg3VXe9/F3DMB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitBerg/Machine_Learning_Study/blob/master/08-Redes%20Neurais%20Artificiais/Machine_Learning_Redes_Neurais_Artificiais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHzdlhQMM4_Q"
      },
      "source": [
        "#REDES NEURAIS ARTIFICIAIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN6yVC4jN6La"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTFGV0bnNEvN"
      },
      "source": [
        "###Base Credit Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96gRIOWDNLPE"
      },
      "source": [
        "import pickle\n",
        "with open('credit.pkl', 'rb') as f:\n",
        "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFlbYKB4Bsau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d61324e0-69a4-4123-c489-e15d4a4f16fc"
      },
      "source": [
        "X_credit_treinamento.shape, y_credit_treinamento.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEq__lEjNxXP",
        "outputId": "649b1c90-2002-40f6-f606-8fa66df9d358"
      },
      "source": [
        "X_credit_teste.shape, y_credit_teste.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvozIGqPTMG8",
        "outputId": "50eee60e-9e54-4fb6-d4bc-7242c55ee286"
      },
      "source": [
        "rede_neural_credit = MLPClassifier(max_iter= 1500, verbose=True, tol=0.0000100,\n",
        "                                   solver='adam', activation= 'relu',\n",
        "                                   hidden_layer_sizes = (2,2))\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.87640455\n",
            "Iteration 2, loss = 0.86327700\n",
            "Iteration 3, loss = 0.85041765\n",
            "Iteration 4, loss = 0.83787501\n",
            "Iteration 5, loss = 0.82552452\n",
            "Iteration 6, loss = 0.81350469\n",
            "Iteration 7, loss = 0.80183608\n",
            "Iteration 8, loss = 0.79058184\n",
            "Iteration 9, loss = 0.77917825\n",
            "Iteration 10, loss = 0.76864452\n",
            "Iteration 11, loss = 0.75811780\n",
            "Iteration 12, loss = 0.74790573\n",
            "Iteration 13, loss = 0.73811664\n",
            "Iteration 14, loss = 0.72848513\n",
            "Iteration 15, loss = 0.71934287\n",
            "Iteration 16, loss = 0.71028041\n",
            "Iteration 17, loss = 0.70150271\n",
            "Iteration 18, loss = 0.69302603\n",
            "Iteration 19, loss = 0.68469348\n",
            "Iteration 20, loss = 0.67669118\n",
            "Iteration 21, loss = 0.66885150\n",
            "Iteration 22, loss = 0.66127328\n",
            "Iteration 23, loss = 0.65390782\n",
            "Iteration 24, loss = 0.64676622\n",
            "Iteration 25, loss = 0.63975992\n",
            "Iteration 26, loss = 0.63313364\n",
            "Iteration 27, loss = 0.62647067\n",
            "Iteration 28, loss = 0.62016266\n",
            "Iteration 29, loss = 0.61392930\n",
            "Iteration 30, loss = 0.60793072\n",
            "Iteration 31, loss = 0.60210516\n",
            "Iteration 32, loss = 0.59636780\n",
            "Iteration 33, loss = 0.59084144\n",
            "Iteration 34, loss = 0.58536661\n",
            "Iteration 35, loss = 0.58016290\n",
            "Iteration 36, loss = 0.57497718\n",
            "Iteration 37, loss = 0.56994524\n",
            "Iteration 38, loss = 0.56500699\n",
            "Iteration 39, loss = 0.56021711\n",
            "Iteration 40, loss = 0.55553013\n",
            "Iteration 41, loss = 0.55111652\n",
            "Iteration 42, loss = 0.54669265\n",
            "Iteration 43, loss = 0.54240748\n",
            "Iteration 44, loss = 0.53828337\n",
            "Iteration 45, loss = 0.53427653\n",
            "Iteration 46, loss = 0.53041642\n",
            "Iteration 47, loss = 0.52663709\n",
            "Iteration 48, loss = 0.52296436\n",
            "Iteration 49, loss = 0.51945402\n",
            "Iteration 50, loss = 0.51605089\n",
            "Iteration 51, loss = 0.51287964\n",
            "Iteration 52, loss = 0.50983574\n",
            "Iteration 53, loss = 0.50687709\n",
            "Iteration 54, loss = 0.50406000\n",
            "Iteration 55, loss = 0.50137686\n",
            "Iteration 56, loss = 0.49873805\n",
            "Iteration 57, loss = 0.49624465\n",
            "Iteration 58, loss = 0.49380430\n",
            "Iteration 59, loss = 0.49154137\n",
            "Iteration 60, loss = 0.48932876\n",
            "Iteration 61, loss = 0.48718092\n",
            "Iteration 62, loss = 0.48507459\n",
            "Iteration 63, loss = 0.48302767\n",
            "Iteration 64, loss = 0.48101359\n",
            "Iteration 65, loss = 0.47906685\n",
            "Iteration 66, loss = 0.47716298\n",
            "Iteration 67, loss = 0.47529056\n",
            "Iteration 68, loss = 0.47347914\n",
            "Iteration 69, loss = 0.47171420\n",
            "Iteration 70, loss = 0.46996763\n",
            "Iteration 71, loss = 0.46826822\n",
            "Iteration 72, loss = 0.46662630\n",
            "Iteration 73, loss = 0.46501231\n",
            "Iteration 74, loss = 0.46347342\n",
            "Iteration 75, loss = 0.46190772\n",
            "Iteration 76, loss = 0.46037402\n",
            "Iteration 77, loss = 0.45888459\n",
            "Iteration 78, loss = 0.45740874\n",
            "Iteration 79, loss = 0.45591924\n",
            "Iteration 80, loss = 0.45451197\n",
            "Iteration 81, loss = 0.45312139\n",
            "Iteration 82, loss = 0.45173094\n",
            "Iteration 83, loss = 0.45039393\n",
            "Iteration 84, loss = 0.44903200\n",
            "Iteration 85, loss = 0.44771423\n",
            "Iteration 86, loss = 0.44643333\n",
            "Iteration 87, loss = 0.44513325\n",
            "Iteration 88, loss = 0.44390657\n",
            "Iteration 89, loss = 0.44267156\n",
            "Iteration 90, loss = 0.44147527\n",
            "Iteration 91, loss = 0.44026960\n",
            "Iteration 92, loss = 0.43909642\n",
            "Iteration 93, loss = 0.43792971\n",
            "Iteration 94, loss = 0.43677604\n",
            "Iteration 95, loss = 0.43563741\n",
            "Iteration 96, loss = 0.43452624\n",
            "Iteration 97, loss = 0.43343281\n",
            "Iteration 98, loss = 0.43232752\n",
            "Iteration 99, loss = 0.43127084\n",
            "Iteration 100, loss = 0.43019356\n",
            "Iteration 101, loss = 0.42914852\n",
            "Iteration 102, loss = 0.42808893\n",
            "Iteration 103, loss = 0.42703695\n",
            "Iteration 104, loss = 0.42601357\n",
            "Iteration 105, loss = 0.42496354\n",
            "Iteration 106, loss = 0.42393095\n",
            "Iteration 107, loss = 0.42285001\n",
            "Iteration 108, loss = 0.42178816\n",
            "Iteration 109, loss = 0.42071876\n",
            "Iteration 110, loss = 0.41966487\n",
            "Iteration 111, loss = 0.41854407\n",
            "Iteration 112, loss = 0.41747389\n",
            "Iteration 113, loss = 0.41640442\n",
            "Iteration 114, loss = 0.41528519\n",
            "Iteration 115, loss = 0.41416882\n",
            "Iteration 116, loss = 0.41306187\n",
            "Iteration 117, loss = 0.41193408\n",
            "Iteration 118, loss = 0.41080861\n",
            "Iteration 119, loss = 0.40970223\n",
            "Iteration 120, loss = 0.40859549\n",
            "Iteration 121, loss = 0.40749901\n",
            "Iteration 122, loss = 0.40643076\n",
            "Iteration 123, loss = 0.40536681\n",
            "Iteration 124, loss = 0.40431203\n",
            "Iteration 125, loss = 0.40329554\n",
            "Iteration 126, loss = 0.40229035\n",
            "Iteration 127, loss = 0.40130314\n",
            "Iteration 128, loss = 0.40033800\n",
            "Iteration 129, loss = 0.39940951\n",
            "Iteration 130, loss = 0.39848321\n",
            "Iteration 131, loss = 0.39755812\n",
            "Iteration 132, loss = 0.39665483\n",
            "Iteration 133, loss = 0.39575137\n",
            "Iteration 134, loss = 0.39484097\n",
            "Iteration 135, loss = 0.39393331\n",
            "Iteration 136, loss = 0.39305833\n",
            "Iteration 137, loss = 0.39218640\n",
            "Iteration 138, loss = 0.39130915\n",
            "Iteration 139, loss = 0.39045523\n",
            "Iteration 140, loss = 0.38957766\n",
            "Iteration 141, loss = 0.38872289\n",
            "Iteration 142, loss = 0.38785968\n",
            "Iteration 143, loss = 0.38699759\n",
            "Iteration 144, loss = 0.38612633\n",
            "Iteration 145, loss = 0.38526652\n",
            "Iteration 146, loss = 0.38440066\n",
            "Iteration 147, loss = 0.38354413\n",
            "Iteration 148, loss = 0.38269686\n",
            "Iteration 149, loss = 0.38183841\n",
            "Iteration 150, loss = 0.38097696\n",
            "Iteration 151, loss = 0.38009950\n",
            "Iteration 152, loss = 0.37923280\n",
            "Iteration 153, loss = 0.37835920\n",
            "Iteration 154, loss = 0.37749572\n",
            "Iteration 155, loss = 0.37663716\n",
            "Iteration 156, loss = 0.37578404\n",
            "Iteration 157, loss = 0.37492865\n",
            "Iteration 158, loss = 0.37407844\n",
            "Iteration 159, loss = 0.37322579\n",
            "Iteration 160, loss = 0.37236655\n",
            "Iteration 161, loss = 0.37150913\n",
            "Iteration 162, loss = 0.37065204\n",
            "Iteration 163, loss = 0.36977281\n",
            "Iteration 164, loss = 0.36890105\n",
            "Iteration 165, loss = 0.36803456\n",
            "Iteration 166, loss = 0.36715104\n",
            "Iteration 167, loss = 0.36628277\n",
            "Iteration 168, loss = 0.36538641\n",
            "Iteration 169, loss = 0.36449842\n",
            "Iteration 170, loss = 0.36361369\n",
            "Iteration 171, loss = 0.36270521\n",
            "Iteration 172, loss = 0.36180041\n",
            "Iteration 173, loss = 0.36089418\n",
            "Iteration 174, loss = 0.35998003\n",
            "Iteration 175, loss = 0.35906215\n",
            "Iteration 176, loss = 0.35814369\n",
            "Iteration 177, loss = 0.35721215\n",
            "Iteration 178, loss = 0.35628177\n",
            "Iteration 179, loss = 0.35534143\n",
            "Iteration 180, loss = 0.35440286\n",
            "Iteration 181, loss = 0.35345662\n",
            "Iteration 182, loss = 0.35251603\n",
            "Iteration 183, loss = 0.35156318\n",
            "Iteration 184, loss = 0.35060107\n",
            "Iteration 185, loss = 0.34964456\n",
            "Iteration 186, loss = 0.34868220\n",
            "Iteration 187, loss = 0.34772132\n",
            "Iteration 188, loss = 0.34674277\n",
            "Iteration 189, loss = 0.34576526\n",
            "Iteration 190, loss = 0.34477732\n",
            "Iteration 191, loss = 0.34379067\n",
            "Iteration 192, loss = 0.34280550\n",
            "Iteration 193, loss = 0.34181096\n",
            "Iteration 194, loss = 0.34078946\n",
            "Iteration 195, loss = 0.33978947\n",
            "Iteration 196, loss = 0.33878065\n",
            "Iteration 197, loss = 0.33775800\n",
            "Iteration 198, loss = 0.33673511\n",
            "Iteration 199, loss = 0.33571667\n",
            "Iteration 200, loss = 0.33468691\n",
            "Iteration 201, loss = 0.33364286\n",
            "Iteration 202, loss = 0.33259106\n",
            "Iteration 203, loss = 0.33155169\n",
            "Iteration 204, loss = 0.33049563\n",
            "Iteration 205, loss = 0.32944592\n",
            "Iteration 206, loss = 0.32838422\n",
            "Iteration 207, loss = 0.32730280\n",
            "Iteration 208, loss = 0.32621587\n",
            "Iteration 209, loss = 0.32515246\n",
            "Iteration 210, loss = 0.32404955\n",
            "Iteration 211, loss = 0.32295594\n",
            "Iteration 212, loss = 0.32184800\n",
            "Iteration 213, loss = 0.32075846\n",
            "Iteration 214, loss = 0.31964941\n",
            "Iteration 215, loss = 0.31852549\n",
            "Iteration 216, loss = 0.31742497\n",
            "Iteration 217, loss = 0.31629910\n",
            "Iteration 218, loss = 0.31515952\n",
            "Iteration 219, loss = 0.31405531\n",
            "Iteration 220, loss = 0.31290572\n",
            "Iteration 221, loss = 0.31179412\n",
            "Iteration 222, loss = 0.31064002\n",
            "Iteration 223, loss = 0.30947948\n",
            "Iteration 224, loss = 0.30835237\n",
            "Iteration 225, loss = 0.30717508\n",
            "Iteration 226, loss = 0.30602826\n",
            "Iteration 227, loss = 0.30483914\n",
            "Iteration 228, loss = 0.30369966\n",
            "Iteration 229, loss = 0.30253385\n",
            "Iteration 230, loss = 0.30129366\n",
            "Iteration 231, loss = 0.30013358\n",
            "Iteration 232, loss = 0.29893555\n",
            "Iteration 233, loss = 0.29778431\n",
            "Iteration 234, loss = 0.29658407\n",
            "Iteration 235, loss = 0.29541576\n",
            "Iteration 236, loss = 0.29421508\n",
            "Iteration 237, loss = 0.29300620\n",
            "Iteration 238, loss = 0.29183137\n",
            "Iteration 239, loss = 0.29061692\n",
            "Iteration 240, loss = 0.28940924\n",
            "Iteration 241, loss = 0.28819490\n",
            "Iteration 242, loss = 0.28697303\n",
            "Iteration 243, loss = 0.28574166\n",
            "Iteration 244, loss = 0.28451984\n",
            "Iteration 245, loss = 0.28330675\n",
            "Iteration 246, loss = 0.28208630\n",
            "Iteration 247, loss = 0.28085198\n",
            "Iteration 248, loss = 0.27962337\n",
            "Iteration 249, loss = 0.27840591\n",
            "Iteration 250, loss = 0.27716505\n",
            "Iteration 251, loss = 0.27596601\n",
            "Iteration 252, loss = 0.27466643\n",
            "Iteration 253, loss = 0.27344149\n",
            "Iteration 254, loss = 0.27223936\n",
            "Iteration 255, loss = 0.27098768\n",
            "Iteration 256, loss = 0.26977474\n",
            "Iteration 257, loss = 0.26853583\n",
            "Iteration 258, loss = 0.26730685\n",
            "Iteration 259, loss = 0.26608187\n",
            "Iteration 260, loss = 0.26484248\n",
            "Iteration 261, loss = 0.26363908\n",
            "Iteration 262, loss = 0.26240969\n",
            "Iteration 263, loss = 0.26118848\n",
            "Iteration 264, loss = 0.25997517\n",
            "Iteration 265, loss = 0.25872964\n",
            "Iteration 266, loss = 0.25751871\n",
            "Iteration 267, loss = 0.25628329\n",
            "Iteration 268, loss = 0.25508238\n",
            "Iteration 269, loss = 0.25385063\n",
            "Iteration 270, loss = 0.25264593\n",
            "Iteration 271, loss = 0.25142370\n",
            "Iteration 272, loss = 0.25024159\n",
            "Iteration 273, loss = 0.24903617\n",
            "Iteration 274, loss = 0.24781213\n",
            "Iteration 275, loss = 0.24661673\n",
            "Iteration 276, loss = 0.24542006\n",
            "Iteration 277, loss = 0.24423078\n",
            "Iteration 278, loss = 0.24302208\n",
            "Iteration 279, loss = 0.24183768\n",
            "Iteration 280, loss = 0.24063863\n",
            "Iteration 281, loss = 0.23944323\n",
            "Iteration 282, loss = 0.23827458\n",
            "Iteration 283, loss = 0.23711048\n",
            "Iteration 284, loss = 0.23592055\n",
            "Iteration 285, loss = 0.23474018\n",
            "Iteration 286, loss = 0.23362386\n",
            "Iteration 287, loss = 0.23244306\n",
            "Iteration 288, loss = 0.23126919\n",
            "Iteration 289, loss = 0.23010730\n",
            "Iteration 290, loss = 0.22895424\n",
            "Iteration 291, loss = 0.22782016\n",
            "Iteration 292, loss = 0.22666793\n",
            "Iteration 293, loss = 0.22552230\n",
            "Iteration 294, loss = 0.22435650\n",
            "Iteration 295, loss = 0.22325531\n",
            "Iteration 296, loss = 0.22209743\n",
            "Iteration 297, loss = 0.22096896\n",
            "Iteration 298, loss = 0.21984698\n",
            "Iteration 299, loss = 0.21873627\n",
            "Iteration 300, loss = 0.21759303\n",
            "Iteration 301, loss = 0.21650945\n",
            "Iteration 302, loss = 0.21538900\n",
            "Iteration 303, loss = 0.21429613\n",
            "Iteration 304, loss = 0.21319966\n",
            "Iteration 305, loss = 0.21211134\n",
            "Iteration 306, loss = 0.21102101\n",
            "Iteration 307, loss = 0.20992632\n",
            "Iteration 308, loss = 0.20885690\n",
            "Iteration 309, loss = 0.20778549\n",
            "Iteration 310, loss = 0.20673253\n",
            "Iteration 311, loss = 0.20566982\n",
            "Iteration 312, loss = 0.20461692\n",
            "Iteration 313, loss = 0.20358423\n",
            "Iteration 314, loss = 0.20254702\n",
            "Iteration 315, loss = 0.20148152\n",
            "Iteration 316, loss = 0.20046091\n",
            "Iteration 317, loss = 0.19945975\n",
            "Iteration 318, loss = 0.19842337\n",
            "Iteration 319, loss = 0.19741398\n",
            "Iteration 320, loss = 0.19640028\n",
            "Iteration 321, loss = 0.19540696\n",
            "Iteration 322, loss = 0.19439601\n",
            "Iteration 323, loss = 0.19340215\n",
            "Iteration 324, loss = 0.19244399\n",
            "Iteration 325, loss = 0.19145934\n",
            "Iteration 326, loss = 0.19044865\n",
            "Iteration 327, loss = 0.18947709\n",
            "Iteration 328, loss = 0.18852400\n",
            "Iteration 329, loss = 0.18753630\n",
            "Iteration 330, loss = 0.18658310\n",
            "Iteration 331, loss = 0.18562416\n",
            "Iteration 332, loss = 0.18468401\n",
            "Iteration 333, loss = 0.18375292\n",
            "Iteration 334, loss = 0.18279718\n",
            "Iteration 335, loss = 0.18189412\n",
            "Iteration 336, loss = 0.18093343\n",
            "Iteration 337, loss = 0.18002155\n",
            "Iteration 338, loss = 0.17912364\n",
            "Iteration 339, loss = 0.17823071\n",
            "Iteration 340, loss = 0.17732188\n",
            "Iteration 341, loss = 0.17642291\n",
            "Iteration 342, loss = 0.17557178\n",
            "Iteration 343, loss = 0.17464437\n",
            "Iteration 344, loss = 0.17376909\n",
            "Iteration 345, loss = 0.17292784\n",
            "Iteration 346, loss = 0.17205380\n",
            "Iteration 347, loss = 0.17118694\n",
            "Iteration 348, loss = 0.17034401\n",
            "Iteration 349, loss = 0.16950792\n",
            "Iteration 350, loss = 0.16864778\n",
            "Iteration 351, loss = 0.16782065\n",
            "Iteration 352, loss = 0.16701247\n",
            "Iteration 353, loss = 0.16618898\n",
            "Iteration 354, loss = 0.16535430\n",
            "Iteration 355, loss = 0.16453721\n",
            "Iteration 356, loss = 0.16374980\n",
            "Iteration 357, loss = 0.16294744\n",
            "Iteration 358, loss = 0.16213707\n",
            "Iteration 359, loss = 0.16133355\n",
            "Iteration 360, loss = 0.16057631\n",
            "Iteration 361, loss = 0.15976870\n",
            "Iteration 362, loss = 0.15899186\n",
            "Iteration 363, loss = 0.15822786\n",
            "Iteration 364, loss = 0.15744903\n",
            "Iteration 365, loss = 0.15671210\n",
            "Iteration 366, loss = 0.15594087\n",
            "Iteration 367, loss = 0.15516834\n",
            "Iteration 368, loss = 0.15443016\n",
            "Iteration 369, loss = 0.15366783\n",
            "Iteration 370, loss = 0.15295445\n",
            "Iteration 371, loss = 0.15220678\n",
            "Iteration 372, loss = 0.15145204\n",
            "Iteration 373, loss = 0.15075533\n",
            "Iteration 374, loss = 0.15001142\n",
            "Iteration 375, loss = 0.14938869\n",
            "Iteration 376, loss = 0.14860106\n",
            "Iteration 377, loss = 0.14789420\n",
            "Iteration 378, loss = 0.14719187\n",
            "Iteration 379, loss = 0.14648151\n",
            "Iteration 380, loss = 0.14580223\n",
            "Iteration 381, loss = 0.14512746\n",
            "Iteration 382, loss = 0.14442760\n",
            "Iteration 383, loss = 0.14373614\n",
            "Iteration 384, loss = 0.14308820\n",
            "Iteration 385, loss = 0.14239930\n",
            "Iteration 386, loss = 0.14173885\n",
            "Iteration 387, loss = 0.14107345\n",
            "Iteration 388, loss = 0.14042546\n",
            "Iteration 389, loss = 0.13975225\n",
            "Iteration 390, loss = 0.13910185\n",
            "Iteration 391, loss = 0.13848792\n",
            "Iteration 392, loss = 0.13778784\n",
            "Iteration 393, loss = 0.13718180\n",
            "Iteration 394, loss = 0.13652888\n",
            "Iteration 395, loss = 0.13591651\n",
            "Iteration 396, loss = 0.13528831\n",
            "Iteration 397, loss = 0.13464228\n",
            "Iteration 398, loss = 0.13407937\n",
            "Iteration 399, loss = 0.13340440\n",
            "Iteration 400, loss = 0.13281074\n",
            "Iteration 401, loss = 0.13220657\n",
            "Iteration 402, loss = 0.13158508\n",
            "Iteration 403, loss = 0.13101807\n",
            "Iteration 404, loss = 0.13039506\n",
            "Iteration 405, loss = 0.12980808\n",
            "Iteration 406, loss = 0.12921207\n",
            "Iteration 407, loss = 0.12866258\n",
            "Iteration 408, loss = 0.12804761\n",
            "Iteration 409, loss = 0.12746806\n",
            "Iteration 410, loss = 0.12690606\n",
            "Iteration 411, loss = 0.12635961\n",
            "Iteration 412, loss = 0.12579105\n",
            "Iteration 413, loss = 0.12522067\n",
            "Iteration 414, loss = 0.12466625\n",
            "Iteration 415, loss = 0.12409585\n",
            "Iteration 416, loss = 0.12357519\n",
            "Iteration 417, loss = 0.12301851\n",
            "Iteration 418, loss = 0.12247371\n",
            "Iteration 419, loss = 0.12194499\n",
            "Iteration 420, loss = 0.12140491\n",
            "Iteration 421, loss = 0.12086768\n",
            "Iteration 422, loss = 0.12036272\n",
            "Iteration 423, loss = 0.11980251\n",
            "Iteration 424, loss = 0.11928488\n",
            "Iteration 425, loss = 0.11875539\n",
            "Iteration 426, loss = 0.11824548\n",
            "Iteration 427, loss = 0.11774165\n",
            "Iteration 428, loss = 0.11724202\n",
            "Iteration 429, loss = 0.11672494\n",
            "Iteration 430, loss = 0.11622875\n",
            "Iteration 431, loss = 0.11574579\n",
            "Iteration 432, loss = 0.11521311\n",
            "Iteration 433, loss = 0.11472286\n",
            "Iteration 434, loss = 0.11422813\n",
            "Iteration 435, loss = 0.11373349\n",
            "Iteration 436, loss = 0.11323824\n",
            "Iteration 437, loss = 0.11278316\n",
            "Iteration 438, loss = 0.11226424\n",
            "Iteration 439, loss = 0.11178634\n",
            "Iteration 440, loss = 0.11131538\n",
            "Iteration 441, loss = 0.11085197\n",
            "Iteration 442, loss = 0.11038042\n",
            "Iteration 443, loss = 0.10992185\n",
            "Iteration 444, loss = 0.10945292\n",
            "Iteration 445, loss = 0.10900601\n",
            "Iteration 446, loss = 0.10852567\n",
            "Iteration 447, loss = 0.10807876\n",
            "Iteration 448, loss = 0.10761742\n",
            "Iteration 449, loss = 0.10715575\n",
            "Iteration 450, loss = 0.10673253\n",
            "Iteration 451, loss = 0.10628333\n",
            "Iteration 452, loss = 0.10584465\n",
            "Iteration 453, loss = 0.10537155\n",
            "Iteration 454, loss = 0.10496232\n",
            "Iteration 455, loss = 0.10451662\n",
            "Iteration 456, loss = 0.10410185\n",
            "Iteration 457, loss = 0.10365532\n",
            "Iteration 458, loss = 0.10323528\n",
            "Iteration 459, loss = 0.10280509\n",
            "Iteration 460, loss = 0.10241686\n",
            "Iteration 461, loss = 0.10200271\n",
            "Iteration 462, loss = 0.10157670\n",
            "Iteration 463, loss = 0.10116386\n",
            "Iteration 464, loss = 0.10071997\n",
            "Iteration 465, loss = 0.10031528\n",
            "Iteration 466, loss = 0.09993199\n",
            "Iteration 467, loss = 0.09952487\n",
            "Iteration 468, loss = 0.09911445\n",
            "Iteration 469, loss = 0.09873023\n",
            "Iteration 470, loss = 0.09832006\n",
            "Iteration 471, loss = 0.09798287\n",
            "Iteration 472, loss = 0.09754774\n",
            "Iteration 473, loss = 0.09713171\n",
            "Iteration 474, loss = 0.09677301\n",
            "Iteration 475, loss = 0.09637191\n",
            "Iteration 476, loss = 0.09599447\n",
            "Iteration 477, loss = 0.09562020\n",
            "Iteration 478, loss = 0.09522222\n",
            "Iteration 479, loss = 0.09484882\n",
            "Iteration 480, loss = 0.09447784\n",
            "Iteration 481, loss = 0.09410800\n",
            "Iteration 482, loss = 0.09374488\n",
            "Iteration 483, loss = 0.09337129\n",
            "Iteration 484, loss = 0.09300294\n",
            "Iteration 485, loss = 0.09261392\n",
            "Iteration 486, loss = 0.09227422\n",
            "Iteration 487, loss = 0.09192493\n",
            "Iteration 488, loss = 0.09158318\n",
            "Iteration 489, loss = 0.09121789\n",
            "Iteration 490, loss = 0.09083169\n",
            "Iteration 491, loss = 0.09049167\n",
            "Iteration 492, loss = 0.09013725\n",
            "Iteration 493, loss = 0.08979116\n",
            "Iteration 494, loss = 0.08946547\n",
            "Iteration 495, loss = 0.08916099\n",
            "Iteration 496, loss = 0.08883743\n",
            "Iteration 497, loss = 0.08846074\n",
            "Iteration 498, loss = 0.08810239\n",
            "Iteration 499, loss = 0.08779347\n",
            "Iteration 500, loss = 0.08745038\n",
            "Iteration 501, loss = 0.08711914\n",
            "Iteration 502, loss = 0.08678989\n",
            "Iteration 503, loss = 0.08645268\n",
            "Iteration 504, loss = 0.08612509\n",
            "Iteration 505, loss = 0.08580704\n",
            "Iteration 506, loss = 0.08548568\n",
            "Iteration 507, loss = 0.08516746\n",
            "Iteration 508, loss = 0.08484488\n",
            "Iteration 509, loss = 0.08455465\n",
            "Iteration 510, loss = 0.08424538\n",
            "Iteration 511, loss = 0.08391616\n",
            "Iteration 512, loss = 0.08361530\n",
            "Iteration 513, loss = 0.08333624\n",
            "Iteration 514, loss = 0.08298609\n",
            "Iteration 515, loss = 0.08270032\n",
            "Iteration 516, loss = 0.08238255\n",
            "Iteration 517, loss = 0.08210909\n",
            "Iteration 518, loss = 0.08181981\n",
            "Iteration 519, loss = 0.08150503\n",
            "Iteration 520, loss = 0.08121894\n",
            "Iteration 521, loss = 0.08091850\n",
            "Iteration 522, loss = 0.08063617\n",
            "Iteration 523, loss = 0.08035533\n",
            "Iteration 524, loss = 0.08003750\n",
            "Iteration 525, loss = 0.07978328\n",
            "Iteration 526, loss = 0.07948136\n",
            "Iteration 527, loss = 0.07918580\n",
            "Iteration 528, loss = 0.07888320\n",
            "Iteration 529, loss = 0.07861454\n",
            "Iteration 530, loss = 0.07830526\n",
            "Iteration 531, loss = 0.07801382\n",
            "Iteration 532, loss = 0.07767689\n",
            "Iteration 533, loss = 0.07739153\n",
            "Iteration 534, loss = 0.07709738\n",
            "Iteration 535, loss = 0.07680437\n",
            "Iteration 536, loss = 0.07650059\n",
            "Iteration 537, loss = 0.07624398\n",
            "Iteration 538, loss = 0.07596218\n",
            "Iteration 539, loss = 0.07568225\n",
            "Iteration 540, loss = 0.07538929\n",
            "Iteration 541, loss = 0.07511872\n",
            "Iteration 542, loss = 0.07483898\n",
            "Iteration 543, loss = 0.07458158\n",
            "Iteration 544, loss = 0.07430106\n",
            "Iteration 545, loss = 0.07403166\n",
            "Iteration 546, loss = 0.07375080\n",
            "Iteration 547, loss = 0.07349109\n",
            "Iteration 548, loss = 0.07320520\n",
            "Iteration 549, loss = 0.07294040\n",
            "Iteration 550, loss = 0.07268109\n",
            "Iteration 551, loss = 0.07241063\n",
            "Iteration 552, loss = 0.07216721\n",
            "Iteration 553, loss = 0.07189131\n",
            "Iteration 554, loss = 0.07167808\n",
            "Iteration 555, loss = 0.07138952\n",
            "Iteration 556, loss = 0.07113437\n",
            "Iteration 557, loss = 0.07087778\n",
            "Iteration 558, loss = 0.07058702\n",
            "Iteration 559, loss = 0.07035444\n",
            "Iteration 560, loss = 0.07011902\n",
            "Iteration 561, loss = 0.06985125\n",
            "Iteration 562, loss = 0.06959638\n",
            "Iteration 563, loss = 0.06934479\n",
            "Iteration 564, loss = 0.06909153\n",
            "Iteration 565, loss = 0.06887806\n",
            "Iteration 566, loss = 0.06862777\n",
            "Iteration 567, loss = 0.06838267\n",
            "Iteration 568, loss = 0.06812761\n",
            "Iteration 569, loss = 0.06789950\n",
            "Iteration 570, loss = 0.06768272\n",
            "Iteration 571, loss = 0.06742008\n",
            "Iteration 572, loss = 0.06719479\n",
            "Iteration 573, loss = 0.06695826\n",
            "Iteration 574, loss = 0.06672304\n",
            "Iteration 575, loss = 0.06647614\n",
            "Iteration 576, loss = 0.06624244\n",
            "Iteration 577, loss = 0.06602451\n",
            "Iteration 578, loss = 0.06578327\n",
            "Iteration 579, loss = 0.06554718\n",
            "Iteration 580, loss = 0.06536588\n",
            "Iteration 581, loss = 0.06509233\n",
            "Iteration 582, loss = 0.06488608\n",
            "Iteration 583, loss = 0.06465428\n",
            "Iteration 584, loss = 0.06442550\n",
            "Iteration 585, loss = 0.06420894\n",
            "Iteration 586, loss = 0.06398148\n",
            "Iteration 587, loss = 0.06375302\n",
            "Iteration 588, loss = 0.06355587\n",
            "Iteration 589, loss = 0.06331973\n",
            "Iteration 590, loss = 0.06309474\n",
            "Iteration 591, loss = 0.06289818\n",
            "Iteration 592, loss = 0.06271586\n",
            "Iteration 593, loss = 0.06246452\n",
            "Iteration 594, loss = 0.06225735\n",
            "Iteration 595, loss = 0.06204875\n",
            "Iteration 596, loss = 0.06184133\n",
            "Iteration 597, loss = 0.06161579\n",
            "Iteration 598, loss = 0.06142708\n",
            "Iteration 599, loss = 0.06121398\n",
            "Iteration 600, loss = 0.06100808\n",
            "Iteration 601, loss = 0.06079511\n",
            "Iteration 602, loss = 0.06058135\n",
            "Iteration 603, loss = 0.06036804\n",
            "Iteration 604, loss = 0.06016903\n",
            "Iteration 605, loss = 0.05996007\n",
            "Iteration 606, loss = 0.05974256\n",
            "Iteration 607, loss = 0.05955170\n",
            "Iteration 608, loss = 0.05938607\n",
            "Iteration 609, loss = 0.05912652\n",
            "Iteration 610, loss = 0.05896226\n",
            "Iteration 611, loss = 0.05870690\n",
            "Iteration 612, loss = 0.05863058\n",
            "Iteration 613, loss = 0.05833253\n",
            "Iteration 614, loss = 0.05814021\n",
            "Iteration 615, loss = 0.05794598\n",
            "Iteration 616, loss = 0.05773649\n",
            "Iteration 617, loss = 0.05754440\n",
            "Iteration 618, loss = 0.05733568\n",
            "Iteration 619, loss = 0.05713713\n",
            "Iteration 620, loss = 0.05694682\n",
            "Iteration 621, loss = 0.05675832\n",
            "Iteration 622, loss = 0.05662190\n",
            "Iteration 623, loss = 0.05642238\n",
            "Iteration 624, loss = 0.05621113\n",
            "Iteration 625, loss = 0.05599799\n",
            "Iteration 626, loss = 0.05581045\n",
            "Iteration 627, loss = 0.05564055\n",
            "Iteration 628, loss = 0.05547405\n",
            "Iteration 629, loss = 0.05524139\n",
            "Iteration 630, loss = 0.05507337\n",
            "Iteration 631, loss = 0.05488750\n",
            "Iteration 632, loss = 0.05468833\n",
            "Iteration 633, loss = 0.05451980\n",
            "Iteration 634, loss = 0.05433549\n",
            "Iteration 635, loss = 0.05415046\n",
            "Iteration 636, loss = 0.05400550\n",
            "Iteration 637, loss = 0.05382047\n",
            "Iteration 638, loss = 0.05361671\n",
            "Iteration 639, loss = 0.05343824\n",
            "Iteration 640, loss = 0.05325027\n",
            "Iteration 641, loss = 0.05308262\n",
            "Iteration 642, loss = 0.05291211\n",
            "Iteration 643, loss = 0.05273746\n",
            "Iteration 644, loss = 0.05256404\n",
            "Iteration 645, loss = 0.05238673\n",
            "Iteration 646, loss = 0.05221413\n",
            "Iteration 647, loss = 0.05205420\n",
            "Iteration 648, loss = 0.05187855\n",
            "Iteration 649, loss = 0.05173247\n",
            "Iteration 650, loss = 0.05154004\n",
            "Iteration 651, loss = 0.05140304\n",
            "Iteration 652, loss = 0.05122181\n",
            "Iteration 653, loss = 0.05103959\n",
            "Iteration 654, loss = 0.05090298\n",
            "Iteration 655, loss = 0.05071144\n",
            "Iteration 656, loss = 0.05056489\n",
            "Iteration 657, loss = 0.05040635\n",
            "Iteration 658, loss = 0.05022219\n",
            "Iteration 659, loss = 0.05006912\n",
            "Iteration 660, loss = 0.04992128\n",
            "Iteration 661, loss = 0.04976265\n",
            "Iteration 662, loss = 0.04960798\n",
            "Iteration 663, loss = 0.04946635\n",
            "Iteration 664, loss = 0.04924909\n",
            "Iteration 665, loss = 0.04911094\n",
            "Iteration 666, loss = 0.04894932\n",
            "Iteration 667, loss = 0.04879461\n",
            "Iteration 668, loss = 0.04864580\n",
            "Iteration 669, loss = 0.04850036\n",
            "Iteration 670, loss = 0.04832397\n",
            "Iteration 671, loss = 0.04816121\n",
            "Iteration 672, loss = 0.04804304\n",
            "Iteration 673, loss = 0.04788837\n",
            "Iteration 674, loss = 0.04774291\n",
            "Iteration 675, loss = 0.04757639\n",
            "Iteration 676, loss = 0.04745269\n",
            "Iteration 677, loss = 0.04725936\n",
            "Iteration 678, loss = 0.04712969\n",
            "Iteration 679, loss = 0.04700800\n",
            "Iteration 680, loss = 0.04682229\n",
            "Iteration 681, loss = 0.04666150\n",
            "Iteration 682, loss = 0.04653163\n",
            "Iteration 683, loss = 0.04638491\n",
            "Iteration 684, loss = 0.04622910\n",
            "Iteration 685, loss = 0.04612351\n",
            "Iteration 686, loss = 0.04599813\n",
            "Iteration 687, loss = 0.04579623\n",
            "Iteration 688, loss = 0.04565396\n",
            "Iteration 689, loss = 0.04551242\n",
            "Iteration 690, loss = 0.04539026\n",
            "Iteration 691, loss = 0.04523916\n",
            "Iteration 692, loss = 0.04508057\n",
            "Iteration 693, loss = 0.04499200\n",
            "Iteration 694, loss = 0.04480385\n",
            "Iteration 695, loss = 0.04471551\n",
            "Iteration 696, loss = 0.04453263\n",
            "Iteration 697, loss = 0.04439052\n",
            "Iteration 698, loss = 0.04427133\n",
            "Iteration 699, loss = 0.04410112\n",
            "Iteration 700, loss = 0.04398496\n",
            "Iteration 701, loss = 0.04384263\n",
            "Iteration 702, loss = 0.04371455\n",
            "Iteration 703, loss = 0.04359816\n",
            "Iteration 704, loss = 0.04346963\n",
            "Iteration 705, loss = 0.04330808\n",
            "Iteration 706, loss = 0.04319014\n",
            "Iteration 707, loss = 0.04305189\n",
            "Iteration 708, loss = 0.04292681\n",
            "Iteration 709, loss = 0.04282672\n",
            "Iteration 710, loss = 0.04265352\n",
            "Iteration 711, loss = 0.04253236\n",
            "Iteration 712, loss = 0.04239051\n",
            "Iteration 713, loss = 0.04227002\n",
            "Iteration 714, loss = 0.04216855\n",
            "Iteration 715, loss = 0.04202520\n",
            "Iteration 716, loss = 0.04186549\n",
            "Iteration 717, loss = 0.04176545\n",
            "Iteration 718, loss = 0.04162657\n",
            "Iteration 719, loss = 0.04150547\n",
            "Iteration 720, loss = 0.04137149\n",
            "Iteration 721, loss = 0.04123345\n",
            "Iteration 722, loss = 0.04110784\n",
            "Iteration 723, loss = 0.04099565\n",
            "Iteration 724, loss = 0.04088397\n",
            "Iteration 725, loss = 0.04075365\n",
            "Iteration 726, loss = 0.04062870\n",
            "Iteration 727, loss = 0.04051346\n",
            "Iteration 728, loss = 0.04037197\n",
            "Iteration 729, loss = 0.04026032\n",
            "Iteration 730, loss = 0.04011737\n",
            "Iteration 731, loss = 0.04001987\n",
            "Iteration 732, loss = 0.03989307\n",
            "Iteration 733, loss = 0.03979543\n",
            "Iteration 734, loss = 0.03965193\n",
            "Iteration 735, loss = 0.03953137\n",
            "Iteration 736, loss = 0.03943754\n",
            "Iteration 737, loss = 0.03929783\n",
            "Iteration 738, loss = 0.03918882\n",
            "Iteration 739, loss = 0.03907420\n",
            "Iteration 740, loss = 0.03893218\n",
            "Iteration 741, loss = 0.03883825\n",
            "Iteration 742, loss = 0.03880498\n",
            "Iteration 743, loss = 0.03861806\n",
            "Iteration 744, loss = 0.03847986\n",
            "Iteration 745, loss = 0.03837280\n",
            "Iteration 746, loss = 0.03826847\n",
            "Iteration 747, loss = 0.03815753\n",
            "Iteration 748, loss = 0.03805281\n",
            "Iteration 749, loss = 0.03792335\n",
            "Iteration 750, loss = 0.03781757\n",
            "Iteration 751, loss = 0.03776959\n",
            "Iteration 752, loss = 0.03761234\n",
            "Iteration 753, loss = 0.03747169\n",
            "Iteration 754, loss = 0.03737071\n",
            "Iteration 755, loss = 0.03726446\n",
            "Iteration 756, loss = 0.03715598\n",
            "Iteration 757, loss = 0.03703494\n",
            "Iteration 758, loss = 0.03691535\n",
            "Iteration 759, loss = 0.03681841\n",
            "Iteration 760, loss = 0.03670006\n",
            "Iteration 761, loss = 0.03660717\n",
            "Iteration 762, loss = 0.03648971\n",
            "Iteration 763, loss = 0.03641992\n",
            "Iteration 764, loss = 0.03631268\n",
            "Iteration 765, loss = 0.03622327\n",
            "Iteration 766, loss = 0.03607116\n",
            "Iteration 767, loss = 0.03598223\n",
            "Iteration 768, loss = 0.03583804\n",
            "Iteration 769, loss = 0.03577746\n",
            "Iteration 770, loss = 0.03568080\n",
            "Iteration 771, loss = 0.03554369\n",
            "Iteration 772, loss = 0.03544949\n",
            "Iteration 773, loss = 0.03532441\n",
            "Iteration 774, loss = 0.03523074\n",
            "Iteration 775, loss = 0.03513909\n",
            "Iteration 776, loss = 0.03504203\n",
            "Iteration 777, loss = 0.03493958\n",
            "Iteration 778, loss = 0.03481736\n",
            "Iteration 779, loss = 0.03473324\n",
            "Iteration 780, loss = 0.03464096\n",
            "Iteration 781, loss = 0.03459002\n",
            "Iteration 782, loss = 0.03443016\n",
            "Iteration 783, loss = 0.03432733\n",
            "Iteration 784, loss = 0.03422038\n",
            "Iteration 785, loss = 0.03415642\n",
            "Iteration 786, loss = 0.03401893\n",
            "Iteration 787, loss = 0.03392251\n",
            "Iteration 788, loss = 0.03383443\n",
            "Iteration 789, loss = 0.03372582\n",
            "Iteration 790, loss = 0.03362860\n",
            "Iteration 791, loss = 0.03352640\n",
            "Iteration 792, loss = 0.03342122\n",
            "Iteration 793, loss = 0.03330386\n",
            "Iteration 794, loss = 0.03321468\n",
            "Iteration 795, loss = 0.03310783\n",
            "Iteration 796, loss = 0.03300803\n",
            "Iteration 797, loss = 0.03292934\n",
            "Iteration 798, loss = 0.03280065\n",
            "Iteration 799, loss = 0.03272587\n",
            "Iteration 800, loss = 0.03261550\n",
            "Iteration 801, loss = 0.03251883\n",
            "Iteration 802, loss = 0.03241146\n",
            "Iteration 803, loss = 0.03233733\n",
            "Iteration 804, loss = 0.03221371\n",
            "Iteration 805, loss = 0.03211541\n",
            "Iteration 806, loss = 0.03203307\n",
            "Iteration 807, loss = 0.03192903\n",
            "Iteration 808, loss = 0.03187428\n",
            "Iteration 809, loss = 0.03172374\n",
            "Iteration 810, loss = 0.03163254\n",
            "Iteration 811, loss = 0.03154811\n",
            "Iteration 812, loss = 0.03146411\n",
            "Iteration 813, loss = 0.03136160\n",
            "Iteration 814, loss = 0.03124859\n",
            "Iteration 815, loss = 0.03115899\n",
            "Iteration 816, loss = 0.03106730\n",
            "Iteration 817, loss = 0.03096234\n",
            "Iteration 818, loss = 0.03088129\n",
            "Iteration 819, loss = 0.03080751\n",
            "Iteration 820, loss = 0.03068834\n",
            "Iteration 821, loss = 0.03060380\n",
            "Iteration 822, loss = 0.03054741\n",
            "Iteration 823, loss = 0.03042644\n",
            "Iteration 824, loss = 0.03032975\n",
            "Iteration 825, loss = 0.03025669\n",
            "Iteration 826, loss = 0.03016424\n",
            "Iteration 827, loss = 0.03008039\n",
            "Iteration 828, loss = 0.02997837\n",
            "Iteration 829, loss = 0.02988450\n",
            "Iteration 830, loss = 0.02979368\n",
            "Iteration 831, loss = 0.02969677\n",
            "Iteration 832, loss = 0.02960033\n",
            "Iteration 833, loss = 0.02951856\n",
            "Iteration 834, loss = 0.02944976\n",
            "Iteration 835, loss = 0.02934096\n",
            "Iteration 836, loss = 0.02925255\n",
            "Iteration 837, loss = 0.02917518\n",
            "Iteration 838, loss = 0.02909453\n",
            "Iteration 839, loss = 0.02903814\n",
            "Iteration 840, loss = 0.02893869\n",
            "Iteration 841, loss = 0.02883806\n",
            "Iteration 842, loss = 0.02874505\n",
            "Iteration 843, loss = 0.02867565\n",
            "Iteration 844, loss = 0.02857859\n",
            "Iteration 845, loss = 0.02850283\n",
            "Iteration 846, loss = 0.02839327\n",
            "Iteration 847, loss = 0.02834507\n",
            "Iteration 848, loss = 0.02823493\n",
            "Iteration 849, loss = 0.02814683\n",
            "Iteration 850, loss = 0.02806062\n",
            "Iteration 851, loss = 0.02798082\n",
            "Iteration 852, loss = 0.02791749\n",
            "Iteration 853, loss = 0.02782798\n",
            "Iteration 854, loss = 0.02772909\n",
            "Iteration 855, loss = 0.02764588\n",
            "Iteration 856, loss = 0.02760983\n",
            "Iteration 857, loss = 0.02750452\n",
            "Iteration 858, loss = 0.02742141\n",
            "Iteration 859, loss = 0.02733840\n",
            "Iteration 860, loss = 0.02726586\n",
            "Iteration 861, loss = 0.02718329\n",
            "Iteration 862, loss = 0.02709117\n",
            "Iteration 863, loss = 0.02705943\n",
            "Iteration 864, loss = 0.02697731\n",
            "Iteration 865, loss = 0.02685255\n",
            "Iteration 866, loss = 0.02680357\n",
            "Iteration 867, loss = 0.02670531\n",
            "Iteration 868, loss = 0.02662261\n",
            "Iteration 869, loss = 0.02654480\n",
            "Iteration 870, loss = 0.02646252\n",
            "Iteration 871, loss = 0.02638483\n",
            "Iteration 872, loss = 0.02631353\n",
            "Iteration 873, loss = 0.02626843\n",
            "Iteration 874, loss = 0.02614210\n",
            "Iteration 875, loss = 0.02606797\n",
            "Iteration 876, loss = 0.02599518\n",
            "Iteration 877, loss = 0.02593977\n",
            "Iteration 878, loss = 0.02587580\n",
            "Iteration 879, loss = 0.02579183\n",
            "Iteration 880, loss = 0.02570312\n",
            "Iteration 881, loss = 0.02562439\n",
            "Iteration 882, loss = 0.02553746\n",
            "Iteration 883, loss = 0.02545683\n",
            "Iteration 884, loss = 0.02542054\n",
            "Iteration 885, loss = 0.02539013\n",
            "Iteration 886, loss = 0.02523683\n",
            "Iteration 887, loss = 0.02519207\n",
            "Iteration 888, loss = 0.02509001\n",
            "Iteration 889, loss = 0.02502731\n",
            "Iteration 890, loss = 0.02499175\n",
            "Iteration 891, loss = 0.02489848\n",
            "Iteration 892, loss = 0.02481398\n",
            "Iteration 893, loss = 0.02473791\n",
            "Iteration 894, loss = 0.02466804\n",
            "Iteration 895, loss = 0.02461463\n",
            "Iteration 896, loss = 0.02453227\n",
            "Iteration 897, loss = 0.02444734\n",
            "Iteration 898, loss = 0.02437069\n",
            "Iteration 899, loss = 0.02433538\n",
            "Iteration 900, loss = 0.02429450\n",
            "Iteration 901, loss = 0.02417573\n",
            "Iteration 902, loss = 0.02409964\n",
            "Iteration 903, loss = 0.02405140\n",
            "Iteration 904, loss = 0.02398482\n",
            "Iteration 905, loss = 0.02389135\n",
            "Iteration 906, loss = 0.02381738\n",
            "Iteration 907, loss = 0.02374338\n",
            "Iteration 908, loss = 0.02368431\n",
            "Iteration 909, loss = 0.02359736\n",
            "Iteration 910, loss = 0.02353258\n",
            "Iteration 911, loss = 0.02349019\n",
            "Iteration 912, loss = 0.02343912\n",
            "Iteration 913, loss = 0.02333347\n",
            "Iteration 914, loss = 0.02328425\n",
            "Iteration 915, loss = 0.02320151\n",
            "Iteration 916, loss = 0.02313805\n",
            "Iteration 917, loss = 0.02307116\n",
            "Iteration 918, loss = 0.02300155\n",
            "Iteration 919, loss = 0.02292946\n",
            "Iteration 920, loss = 0.02285935\n",
            "Iteration 921, loss = 0.02279388\n",
            "Iteration 922, loss = 0.02273273\n",
            "Iteration 923, loss = 0.02267120\n",
            "Iteration 924, loss = 0.02261917\n",
            "Iteration 925, loss = 0.02255548\n",
            "Iteration 926, loss = 0.02245152\n",
            "Iteration 927, loss = 0.02240580\n",
            "Iteration 928, loss = 0.02234390\n",
            "Iteration 929, loss = 0.02227657\n",
            "Iteration 930, loss = 0.02221191\n",
            "Iteration 931, loss = 0.02214198\n",
            "Iteration 932, loss = 0.02207352\n",
            "Iteration 933, loss = 0.02203341\n",
            "Iteration 934, loss = 0.02198366\n",
            "Iteration 935, loss = 0.02188295\n",
            "Iteration 936, loss = 0.02185356\n",
            "Iteration 937, loss = 0.02176214\n",
            "Iteration 938, loss = 0.02172347\n",
            "Iteration 939, loss = 0.02163065\n",
            "Iteration 940, loss = 0.02157280\n",
            "Iteration 941, loss = 0.02152446\n",
            "Iteration 942, loss = 0.02148785\n",
            "Iteration 943, loss = 0.02142762\n",
            "Iteration 944, loss = 0.02137404\n",
            "Iteration 945, loss = 0.02130902\n",
            "Iteration 946, loss = 0.02121005\n",
            "Iteration 947, loss = 0.02115480\n",
            "Iteration 948, loss = 0.02110527\n",
            "Iteration 949, loss = 0.02105049\n",
            "Iteration 950, loss = 0.02099244\n",
            "Iteration 951, loss = 0.02090630\n",
            "Iteration 952, loss = 0.02086177\n",
            "Iteration 953, loss = 0.02083436\n",
            "Iteration 954, loss = 0.02073003\n",
            "Iteration 955, loss = 0.02066792\n",
            "Iteration 956, loss = 0.02061868\n",
            "Iteration 957, loss = 0.02055643\n",
            "Iteration 958, loss = 0.02051371\n",
            "Iteration 959, loss = 0.02044782\n",
            "Iteration 960, loss = 0.02040891\n",
            "Iteration 961, loss = 0.02032718\n",
            "Iteration 962, loss = 0.02029436\n",
            "Iteration 963, loss = 0.02022582\n",
            "Iteration 964, loss = 0.02014929\n",
            "Iteration 965, loss = 0.02012254\n",
            "Iteration 966, loss = 0.02004362\n",
            "Iteration 967, loss = 0.01997672\n",
            "Iteration 968, loss = 0.01992148\n",
            "Iteration 969, loss = 0.01989667\n",
            "Iteration 970, loss = 0.01983646\n",
            "Iteration 971, loss = 0.01977526\n",
            "Iteration 972, loss = 0.01973550\n",
            "Iteration 973, loss = 0.01964053\n",
            "Iteration 974, loss = 0.01957127\n",
            "Iteration 975, loss = 0.01951811\n",
            "Iteration 976, loss = 0.01949438\n",
            "Iteration 977, loss = 0.01941095\n",
            "Iteration 978, loss = 0.01935661\n",
            "Iteration 979, loss = 0.01930603\n",
            "Iteration 980, loss = 0.01926441\n",
            "Iteration 981, loss = 0.01926172\n",
            "Iteration 982, loss = 0.01912431\n",
            "Iteration 983, loss = 0.01908754\n",
            "Iteration 984, loss = 0.01903183\n",
            "Iteration 985, loss = 0.01898267\n",
            "Iteration 986, loss = 0.01892801\n",
            "Iteration 987, loss = 0.01889508\n",
            "Iteration 988, loss = 0.01881703\n",
            "Iteration 989, loss = 0.01875227\n",
            "Iteration 990, loss = 0.01870956\n",
            "Iteration 991, loss = 0.01865819\n",
            "Iteration 992, loss = 0.01859953\n",
            "Iteration 993, loss = 0.01854913\n",
            "Iteration 994, loss = 0.01855859\n",
            "Iteration 995, loss = 0.01845987\n",
            "Iteration 996, loss = 0.01839245\n",
            "Iteration 997, loss = 0.01832607\n",
            "Iteration 998, loss = 0.01830093\n",
            "Iteration 999, loss = 0.01823723\n",
            "Iteration 1000, loss = 0.01823337\n",
            "Iteration 1001, loss = 0.01815418\n",
            "Iteration 1002, loss = 0.01808929\n",
            "Iteration 1003, loss = 0.01803060\n",
            "Iteration 1004, loss = 0.01798347\n",
            "Iteration 1005, loss = 0.01793808\n",
            "Iteration 1006, loss = 0.01787395\n",
            "Iteration 1007, loss = 0.01784214\n",
            "Iteration 1008, loss = 0.01779944\n",
            "Iteration 1009, loss = 0.01773747\n",
            "Iteration 1010, loss = 0.01771460\n",
            "Iteration 1011, loss = 0.01763432\n",
            "Iteration 1012, loss = 0.01759239\n",
            "Iteration 1013, loss = 0.01754125\n",
            "Iteration 1014, loss = 0.01749887\n",
            "Iteration 1015, loss = 0.01743083\n",
            "Iteration 1016, loss = 0.01739929\n",
            "Iteration 1017, loss = 0.01734848\n",
            "Iteration 1018, loss = 0.01727597\n",
            "Iteration 1019, loss = 0.01723168\n",
            "Iteration 1020, loss = 0.01718542\n",
            "Iteration 1021, loss = 0.01715271\n",
            "Iteration 1022, loss = 0.01709081\n",
            "Iteration 1023, loss = 0.01704101\n",
            "Iteration 1024, loss = 0.01701498\n",
            "Iteration 1025, loss = 0.01695089\n",
            "Iteration 1026, loss = 0.01689992\n",
            "Iteration 1027, loss = 0.01685471\n",
            "Iteration 1028, loss = 0.01681303\n",
            "Iteration 1029, loss = 0.01676365\n",
            "Iteration 1030, loss = 0.01671686\n",
            "Iteration 1031, loss = 0.01666132\n",
            "Iteration 1032, loss = 0.01664376\n",
            "Iteration 1033, loss = 0.01657398\n",
            "Iteration 1034, loss = 0.01652845\n",
            "Iteration 1035, loss = 0.01647379\n",
            "Iteration 1036, loss = 0.01644649\n",
            "Iteration 1037, loss = 0.01639006\n",
            "Iteration 1038, loss = 0.01633679\n",
            "Iteration 1039, loss = 0.01630584\n",
            "Iteration 1040, loss = 0.01625735\n",
            "Iteration 1041, loss = 0.01620820\n",
            "Iteration 1042, loss = 0.01614078\n",
            "Iteration 1043, loss = 0.01611440\n",
            "Iteration 1044, loss = 0.01608259\n",
            "Iteration 1045, loss = 0.01604661\n",
            "Iteration 1046, loss = 0.01597390\n",
            "Iteration 1047, loss = 0.01593844\n",
            "Iteration 1048, loss = 0.01591541\n",
            "Iteration 1049, loss = 0.01587858\n",
            "Iteration 1050, loss = 0.01580082\n",
            "Iteration 1051, loss = 0.01580078\n",
            "Iteration 1052, loss = 0.01571443\n",
            "Iteration 1053, loss = 0.01566474\n",
            "Iteration 1054, loss = 0.01562174\n",
            "Iteration 1055, loss = 0.01557723\n",
            "Iteration 1056, loss = 0.01553368\n",
            "Iteration 1057, loss = 0.01548499\n",
            "Iteration 1058, loss = 0.01543104\n",
            "Iteration 1059, loss = 0.01542113\n",
            "Iteration 1060, loss = 0.01535121\n",
            "Iteration 1061, loss = 0.01530834\n",
            "Iteration 1062, loss = 0.01527101\n",
            "Iteration 1063, loss = 0.01526430\n",
            "Iteration 1064, loss = 0.01517653\n",
            "Iteration 1065, loss = 0.01512818\n",
            "Iteration 1066, loss = 0.01510689\n",
            "Iteration 1067, loss = 0.01506940\n",
            "Iteration 1068, loss = 0.01503690\n",
            "Iteration 1069, loss = 0.01497515\n",
            "Iteration 1070, loss = 0.01495770\n",
            "Iteration 1071, loss = 0.01492197\n",
            "Iteration 1072, loss = 0.01486348\n",
            "Iteration 1073, loss = 0.01482303\n",
            "Iteration 1074, loss = 0.01476581\n",
            "Iteration 1075, loss = 0.01472641\n",
            "Iteration 1076, loss = 0.01468944\n",
            "Iteration 1077, loss = 0.01465592\n",
            "Iteration 1078, loss = 0.01459955\n",
            "Iteration 1079, loss = 0.01456630\n",
            "Iteration 1080, loss = 0.01456101\n",
            "Iteration 1081, loss = 0.01449676\n",
            "Iteration 1082, loss = 0.01443568\n",
            "Iteration 1083, loss = 0.01440403\n",
            "Iteration 1084, loss = 0.01436771\n",
            "Iteration 1085, loss = 0.01431390\n",
            "Iteration 1086, loss = 0.01429742\n",
            "Iteration 1087, loss = 0.01426586\n",
            "Iteration 1088, loss = 0.01420836\n",
            "Iteration 1089, loss = 0.01421185\n",
            "Iteration 1090, loss = 0.01411498\n",
            "Iteration 1091, loss = 0.01409514\n",
            "Iteration 1092, loss = 0.01404990\n",
            "Iteration 1093, loss = 0.01400444\n",
            "Iteration 1094, loss = 0.01397220\n",
            "Iteration 1095, loss = 0.01394766\n",
            "Iteration 1096, loss = 0.01388713\n",
            "Iteration 1097, loss = 0.01383930\n",
            "Iteration 1098, loss = 0.01380216\n",
            "Iteration 1099, loss = 0.01378409\n",
            "Iteration 1100, loss = 0.01371882\n",
            "Iteration 1101, loss = 0.01369189\n",
            "Iteration 1102, loss = 0.01365142\n",
            "Iteration 1103, loss = 0.01362548\n",
            "Iteration 1104, loss = 0.01358954\n",
            "Iteration 1105, loss = 0.01356616\n",
            "Iteration 1106, loss = 0.01353951\n",
            "Iteration 1107, loss = 0.01348741\n",
            "Iteration 1108, loss = 0.01343566\n",
            "Iteration 1109, loss = 0.01338818\n",
            "Iteration 1110, loss = 0.01338986\n",
            "Iteration 1111, loss = 0.01333256\n",
            "Iteration 1112, loss = 0.01329803\n",
            "Iteration 1113, loss = 0.01326227\n",
            "Iteration 1114, loss = 0.01321561\n",
            "Iteration 1115, loss = 0.01318129\n",
            "Iteration 1116, loss = 0.01314974\n",
            "Iteration 1117, loss = 0.01314477\n",
            "Iteration 1118, loss = 0.01307399\n",
            "Iteration 1119, loss = 0.01304294\n",
            "Iteration 1120, loss = 0.01300374\n",
            "Iteration 1121, loss = 0.01297600\n",
            "Iteration 1122, loss = 0.01293860\n",
            "Iteration 1123, loss = 0.01289079\n",
            "Iteration 1124, loss = 0.01284858\n",
            "Iteration 1125, loss = 0.01281682\n",
            "Iteration 1126, loss = 0.01279541\n",
            "Iteration 1127, loss = 0.01275422\n",
            "Iteration 1128, loss = 0.01272637\n",
            "Iteration 1129, loss = 0.01268610\n",
            "Iteration 1130, loss = 0.01266470\n",
            "Iteration 1131, loss = 0.01260472\n",
            "Iteration 1132, loss = 0.01256862\n",
            "Iteration 1133, loss = 0.01254783\n",
            "Iteration 1134, loss = 0.01251667\n",
            "Iteration 1135, loss = 0.01247842\n",
            "Iteration 1136, loss = 0.01245862\n",
            "Iteration 1137, loss = 0.01241261\n",
            "Iteration 1138, loss = 0.01238401\n",
            "Iteration 1139, loss = 0.01235597\n",
            "Iteration 1140, loss = 0.01230440\n",
            "Iteration 1141, loss = 0.01229549\n",
            "Iteration 1142, loss = 0.01224741\n",
            "Iteration 1143, loss = 0.01223803\n",
            "Iteration 1144, loss = 0.01217981\n",
            "Iteration 1145, loss = 0.01215166\n",
            "Iteration 1146, loss = 0.01212417\n",
            "Iteration 1147, loss = 0.01207676\n",
            "Iteration 1148, loss = 0.01207261\n",
            "Iteration 1149, loss = 0.01200424\n",
            "Iteration 1150, loss = 0.01204340\n",
            "Iteration 1151, loss = 0.01197935\n",
            "Iteration 1152, loss = 0.01191376\n",
            "Iteration 1153, loss = 0.01189266\n",
            "Iteration 1154, loss = 0.01185354\n",
            "Iteration 1155, loss = 0.01182147\n",
            "Iteration 1156, loss = 0.01180015\n",
            "Iteration 1157, loss = 0.01177020\n",
            "Iteration 1158, loss = 0.01173404\n",
            "Iteration 1159, loss = 0.01170522\n",
            "Iteration 1160, loss = 0.01166552\n",
            "Iteration 1161, loss = 0.01163682\n",
            "Iteration 1162, loss = 0.01162993\n",
            "Iteration 1163, loss = 0.01159251\n",
            "Iteration 1164, loss = 0.01153677\n",
            "Iteration 1165, loss = 0.01151679\n",
            "Iteration 1166, loss = 0.01149804\n",
            "Iteration 1167, loss = 0.01145264\n",
            "Iteration 1168, loss = 0.01141559\n",
            "Iteration 1169, loss = 0.01139489\n",
            "Iteration 1170, loss = 0.01137194\n",
            "Iteration 1171, loss = 0.01132493\n",
            "Iteration 1172, loss = 0.01132109\n",
            "Iteration 1173, loss = 0.01129539\n",
            "Iteration 1174, loss = 0.01125339\n",
            "Iteration 1175, loss = 0.01121801\n",
            "Iteration 1176, loss = 0.01118173\n",
            "Iteration 1177, loss = 0.01115452\n",
            "Iteration 1178, loss = 0.01111883\n",
            "Iteration 1179, loss = 0.01109864\n",
            "Iteration 1180, loss = 0.01106145\n",
            "Iteration 1181, loss = 0.01102767\n",
            "Iteration 1182, loss = 0.01101450\n",
            "Iteration 1183, loss = 0.01098374\n",
            "Iteration 1184, loss = 0.01094959\n",
            "Iteration 1185, loss = 0.01092397\n",
            "Iteration 1186, loss = 0.01089746\n",
            "Iteration 1187, loss = 0.01085169\n",
            "Iteration 1188, loss = 0.01083803\n",
            "Iteration 1189, loss = 0.01080340\n",
            "Iteration 1190, loss = 0.01077844\n",
            "Iteration 1191, loss = 0.01076325\n",
            "Iteration 1192, loss = 0.01073417\n",
            "Iteration 1193, loss = 0.01068918\n",
            "Iteration 1194, loss = 0.01067624\n",
            "Iteration 1195, loss = 0.01063461\n",
            "Iteration 1196, loss = 0.01060871\n",
            "Iteration 1197, loss = 0.01059211\n",
            "Iteration 1198, loss = 0.01055293\n",
            "Iteration 1199, loss = 0.01051264\n",
            "Iteration 1200, loss = 0.01050651\n",
            "Iteration 1201, loss = 0.01048416\n",
            "Iteration 1202, loss = 0.01044877\n",
            "Iteration 1203, loss = 0.01044945\n",
            "Iteration 1204, loss = 0.01038697\n",
            "Iteration 1205, loss = 0.01037266\n",
            "Iteration 1206, loss = 0.01033344\n",
            "Iteration 1207, loss = 0.01030466\n",
            "Iteration 1208, loss = 0.01028108\n",
            "Iteration 1209, loss = 0.01026370\n",
            "Iteration 1210, loss = 0.01023063\n",
            "Iteration 1211, loss = 0.01020778\n",
            "Iteration 1212, loss = 0.01017195\n",
            "Iteration 1213, loss = 0.01014573\n",
            "Iteration 1214, loss = 0.01010788\n",
            "Iteration 1215, loss = 0.01012122\n",
            "Iteration 1216, loss = 0.01006417\n",
            "Iteration 1217, loss = 0.01005738\n",
            "Iteration 1218, loss = 0.01000933\n",
            "Iteration 1219, loss = 0.01000380\n",
            "Iteration 1220, loss = 0.00998954\n",
            "Iteration 1221, loss = 0.00994995\n",
            "Iteration 1222, loss = 0.00992192\n",
            "Iteration 1223, loss = 0.00988535\n",
            "Iteration 1224, loss = 0.00986928\n",
            "Iteration 1225, loss = 0.00987259\n",
            "Iteration 1226, loss = 0.00982093\n",
            "Iteration 1227, loss = 0.00978582\n",
            "Iteration 1228, loss = 0.00976202\n",
            "Iteration 1229, loss = 0.00972929\n",
            "Iteration 1230, loss = 0.00970723\n",
            "Iteration 1231, loss = 0.00968465\n",
            "Iteration 1232, loss = 0.00968570\n",
            "Iteration 1233, loss = 0.00964880\n",
            "Iteration 1234, loss = 0.00963073\n",
            "Iteration 1235, loss = 0.00960342\n",
            "Iteration 1236, loss = 0.00957906\n",
            "Iteration 1237, loss = 0.00953578\n",
            "Iteration 1238, loss = 0.00955126\n",
            "Iteration 1239, loss = 0.00950355\n",
            "Iteration 1240, loss = 0.00947271\n",
            "Iteration 1241, loss = 0.00945746\n",
            "Iteration 1242, loss = 0.00941989\n",
            "Iteration 1243, loss = 0.00940463\n",
            "Iteration 1244, loss = 0.00938200\n",
            "Iteration 1245, loss = 0.00935061\n",
            "Iteration 1246, loss = 0.00933879\n",
            "Iteration 1247, loss = 0.00929939\n",
            "Iteration 1248, loss = 0.00929305\n",
            "Iteration 1249, loss = 0.00929073\n",
            "Iteration 1250, loss = 0.00925543\n",
            "Iteration 1251, loss = 0.00920820\n",
            "Iteration 1252, loss = 0.00919778\n",
            "Iteration 1253, loss = 0.00916686\n",
            "Iteration 1254, loss = 0.00914545\n",
            "Iteration 1255, loss = 0.00913864\n",
            "Iteration 1256, loss = 0.00914982\n",
            "Iteration 1257, loss = 0.00914498\n",
            "Iteration 1258, loss = 0.00906039\n",
            "Iteration 1259, loss = 0.00903384\n",
            "Iteration 1260, loss = 0.00902172\n",
            "Iteration 1261, loss = 0.00899690\n",
            "Iteration 1262, loss = 0.00894888\n",
            "Iteration 1263, loss = 0.00893479\n",
            "Iteration 1264, loss = 0.00891902\n",
            "Iteration 1265, loss = 0.00888971\n",
            "Iteration 1266, loss = 0.00887642\n",
            "Iteration 1267, loss = 0.00883941\n",
            "Iteration 1268, loss = 0.00884452\n",
            "Iteration 1269, loss = 0.00882356\n",
            "Iteration 1270, loss = 0.00878900\n",
            "Iteration 1271, loss = 0.00876161\n",
            "Iteration 1272, loss = 0.00873342\n",
            "Iteration 1273, loss = 0.00873159\n",
            "Iteration 1274, loss = 0.00870614\n",
            "Iteration 1275, loss = 0.00869412\n",
            "Iteration 1276, loss = 0.00866082\n",
            "Iteration 1277, loss = 0.00863259\n",
            "Iteration 1278, loss = 0.00864974\n",
            "Iteration 1279, loss = 0.00858585\n",
            "Iteration 1280, loss = 0.00855640\n",
            "Iteration 1281, loss = 0.00854180\n",
            "Iteration 1282, loss = 0.00853128\n",
            "Iteration 1283, loss = 0.00850499\n",
            "Iteration 1284, loss = 0.00849774\n",
            "Iteration 1285, loss = 0.00851640\n",
            "Iteration 1286, loss = 0.00844039\n",
            "Iteration 1287, loss = 0.00843074\n",
            "Iteration 1288, loss = 0.00840884\n",
            "Iteration 1289, loss = 0.00838917\n",
            "Iteration 1290, loss = 0.00836807\n",
            "Iteration 1291, loss = 0.00834724\n",
            "Iteration 1292, loss = 0.00835924\n",
            "Iteration 1293, loss = 0.00829712\n",
            "Iteration 1294, loss = 0.00827189\n",
            "Iteration 1295, loss = 0.00829196\n",
            "Iteration 1296, loss = 0.00823840\n",
            "Iteration 1297, loss = 0.00822635\n",
            "Iteration 1298, loss = 0.00819866\n",
            "Iteration 1299, loss = 0.00816650\n",
            "Iteration 1300, loss = 0.00816205\n",
            "Iteration 1301, loss = 0.00814052\n",
            "Iteration 1302, loss = 0.00812548\n",
            "Iteration 1303, loss = 0.00811447\n",
            "Iteration 1304, loss = 0.00808452\n",
            "Iteration 1305, loss = 0.00805552\n",
            "Iteration 1306, loss = 0.00804059\n",
            "Iteration 1307, loss = 0.00802342\n",
            "Iteration 1308, loss = 0.00800767\n",
            "Iteration 1309, loss = 0.00797497\n",
            "Iteration 1310, loss = 0.00796622\n",
            "Iteration 1311, loss = 0.00795387\n",
            "Iteration 1312, loss = 0.00793138\n",
            "Iteration 1313, loss = 0.00792650\n",
            "Iteration 1314, loss = 0.00794502\n",
            "Iteration 1315, loss = 0.00792991\n",
            "Iteration 1316, loss = 0.00787754\n",
            "Iteration 1317, loss = 0.00783345\n",
            "Iteration 1318, loss = 0.00782000\n",
            "Iteration 1319, loss = 0.00780009\n",
            "Iteration 1320, loss = 0.00778751\n",
            "Iteration 1321, loss = 0.00778189\n",
            "Iteration 1322, loss = 0.00775875\n",
            "Iteration 1323, loss = 0.00774227\n",
            "Iteration 1324, loss = 0.00773772\n",
            "Iteration 1325, loss = 0.00769089\n",
            "Iteration 1326, loss = 0.00770010\n",
            "Iteration 1327, loss = 0.00768298\n",
            "Iteration 1328, loss = 0.00765439\n",
            "Iteration 1329, loss = 0.00761141\n",
            "Iteration 1330, loss = 0.00761220\n",
            "Iteration 1331, loss = 0.00757505\n",
            "Iteration 1332, loss = 0.00755933\n",
            "Iteration 1333, loss = 0.00755786\n",
            "Iteration 1334, loss = 0.00753090\n",
            "Iteration 1335, loss = 0.00750610\n",
            "Iteration 1336, loss = 0.00752806\n",
            "Iteration 1337, loss = 0.00746550\n",
            "Iteration 1338, loss = 0.00745817\n",
            "Iteration 1339, loss = 0.00744744\n",
            "Iteration 1340, loss = 0.00741659\n",
            "Iteration 1341, loss = 0.00740920\n",
            "Iteration 1342, loss = 0.00739150\n",
            "Iteration 1343, loss = 0.00737719\n",
            "Iteration 1344, loss = 0.00736784\n",
            "Iteration 1345, loss = 0.00737551\n",
            "Iteration 1346, loss = 0.00731459\n",
            "Iteration 1347, loss = 0.00732385\n",
            "Iteration 1348, loss = 0.00727873\n",
            "Iteration 1349, loss = 0.00727762\n",
            "Iteration 1350, loss = 0.00725499\n",
            "Iteration 1351, loss = 0.00724271\n",
            "Iteration 1352, loss = 0.00723662\n",
            "Iteration 1353, loss = 0.00723859\n",
            "Iteration 1354, loss = 0.00719514\n",
            "Iteration 1355, loss = 0.00719109\n",
            "Iteration 1356, loss = 0.00720853\n",
            "Iteration 1357, loss = 0.00714751\n",
            "Iteration 1358, loss = 0.00712869\n",
            "Iteration 1359, loss = 0.00711958\n",
            "Iteration 1360, loss = 0.00708790\n",
            "Iteration 1361, loss = 0.00706798\n",
            "Iteration 1362, loss = 0.00708432\n",
            "Iteration 1363, loss = 0.00704608\n",
            "Iteration 1364, loss = 0.00702496\n",
            "Iteration 1365, loss = 0.00699118\n",
            "Iteration 1366, loss = 0.00704779\n",
            "Iteration 1367, loss = 0.00700865\n",
            "Iteration 1368, loss = 0.00697158\n",
            "Iteration 1369, loss = 0.00693557\n",
            "Iteration 1370, loss = 0.00703094\n",
            "Iteration 1371, loss = 0.00697857\n",
            "Iteration 1372, loss = 0.00691704\n",
            "Iteration 1373, loss = 0.00688704\n",
            "Iteration 1374, loss = 0.00686589\n",
            "Iteration 1375, loss = 0.00685212\n",
            "Iteration 1376, loss = 0.00683615\n",
            "Iteration 1377, loss = 0.00683163\n",
            "Iteration 1378, loss = 0.00681434\n",
            "Iteration 1379, loss = 0.00680561\n",
            "Iteration 1380, loss = 0.00679789\n",
            "Iteration 1381, loss = 0.00677395\n",
            "Iteration 1382, loss = 0.00675445\n",
            "Iteration 1383, loss = 0.00674593\n",
            "Iteration 1384, loss = 0.00674072\n",
            "Iteration 1385, loss = 0.00671772\n",
            "Iteration 1386, loss = 0.00670677\n",
            "Iteration 1387, loss = 0.00670442\n",
            "Iteration 1388, loss = 0.00665592\n",
            "Iteration 1389, loss = 0.00667195\n",
            "Iteration 1390, loss = 0.00662858\n",
            "Iteration 1391, loss = 0.00662418\n",
            "Iteration 1392, loss = 0.00659604\n",
            "Iteration 1393, loss = 0.00658118\n",
            "Iteration 1394, loss = 0.00656262\n",
            "Iteration 1395, loss = 0.00654188\n",
            "Iteration 1396, loss = 0.00654270\n",
            "Iteration 1397, loss = 0.00654006\n",
            "Iteration 1398, loss = 0.00651670\n",
            "Iteration 1399, loss = 0.00650826\n",
            "Iteration 1400, loss = 0.00649649\n",
            "Iteration 1401, loss = 0.00647248\n",
            "Iteration 1402, loss = 0.00646155\n",
            "Iteration 1403, loss = 0.00644939\n",
            "Iteration 1404, loss = 0.00643837\n",
            "Iteration 1405, loss = 0.00643040\n",
            "Iteration 1406, loss = 0.00643018\n",
            "Iteration 1407, loss = 0.00638938\n",
            "Iteration 1408, loss = 0.00639056\n",
            "Iteration 1409, loss = 0.00637634\n",
            "Iteration 1410, loss = 0.00634413\n",
            "Iteration 1411, loss = 0.00632285\n",
            "Iteration 1412, loss = 0.00631693\n",
            "Iteration 1413, loss = 0.00629481\n",
            "Iteration 1414, loss = 0.00628822\n",
            "Iteration 1415, loss = 0.00627776\n",
            "Iteration 1416, loss = 0.00626855\n",
            "Iteration 1417, loss = 0.00623680\n",
            "Iteration 1418, loss = 0.00625343\n",
            "Iteration 1419, loss = 0.00623162\n",
            "Iteration 1420, loss = 0.00620817\n",
            "Iteration 1421, loss = 0.00619210\n",
            "Iteration 1422, loss = 0.00619989\n",
            "Iteration 1423, loss = 0.00616543\n",
            "Iteration 1424, loss = 0.00615777\n",
            "Iteration 1425, loss = 0.00614240\n",
            "Iteration 1426, loss = 0.00613185\n",
            "Iteration 1427, loss = 0.00612625\n",
            "Iteration 1428, loss = 0.00610112\n",
            "Iteration 1429, loss = 0.00612924\n",
            "Iteration 1430, loss = 0.00608270\n",
            "Iteration 1431, loss = 0.00607296\n",
            "Iteration 1432, loss = 0.00605236\n",
            "Iteration 1433, loss = 0.00603678\n",
            "Iteration 1434, loss = 0.00603298\n",
            "Iteration 1435, loss = 0.00601950\n",
            "Iteration 1436, loss = 0.00599900\n",
            "Iteration 1437, loss = 0.00598028\n",
            "Iteration 1438, loss = 0.00597219\n",
            "Iteration 1439, loss = 0.00596331\n",
            "Iteration 1440, loss = 0.00594120\n",
            "Iteration 1441, loss = 0.00595654\n",
            "Iteration 1442, loss = 0.00591921\n",
            "Iteration 1443, loss = 0.00591831\n",
            "Iteration 1444, loss = 0.00589132\n",
            "Iteration 1445, loss = 0.00588127\n",
            "Iteration 1446, loss = 0.00586077\n",
            "Iteration 1447, loss = 0.00586132\n",
            "Iteration 1448, loss = 0.00582312\n",
            "Iteration 1449, loss = 0.00581727\n",
            "Iteration 1450, loss = 0.00580387\n",
            "Iteration 1451, loss = 0.00579904\n",
            "Iteration 1452, loss = 0.00576207\n",
            "Iteration 1453, loss = 0.00574587\n",
            "Iteration 1454, loss = 0.00573836\n",
            "Iteration 1455, loss = 0.00571736\n",
            "Iteration 1456, loss = 0.00571231\n",
            "Iteration 1457, loss = 0.00567993\n",
            "Iteration 1458, loss = 0.00567706\n",
            "Iteration 1459, loss = 0.00566262\n",
            "Iteration 1460, loss = 0.00565788\n",
            "Iteration 1461, loss = 0.00562819\n",
            "Iteration 1462, loss = 0.00561149\n",
            "Iteration 1463, loss = 0.00562998\n",
            "Iteration 1464, loss = 0.00560532\n",
            "Iteration 1465, loss = 0.00559883\n",
            "Iteration 1466, loss = 0.00559221\n",
            "Iteration 1467, loss = 0.00553924\n",
            "Iteration 1468, loss = 0.00553182\n",
            "Iteration 1469, loss = 0.00551812\n",
            "Iteration 1470, loss = 0.00550411\n",
            "Iteration 1471, loss = 0.00548502\n",
            "Iteration 1472, loss = 0.00547409\n",
            "Iteration 1473, loss = 0.00547103\n",
            "Iteration 1474, loss = 0.00550786\n",
            "Iteration 1475, loss = 0.00545232\n",
            "Iteration 1476, loss = 0.00543856\n",
            "Iteration 1477, loss = 0.00540853\n",
            "Iteration 1478, loss = 0.00538758\n",
            "Iteration 1479, loss = 0.00537409\n",
            "Iteration 1480, loss = 0.00536545\n",
            "Iteration 1481, loss = 0.00535116\n",
            "Iteration 1482, loss = 0.00537356\n",
            "Iteration 1483, loss = 0.00532814\n",
            "Iteration 1484, loss = 0.00531308\n",
            "Iteration 1485, loss = 0.00529519\n",
            "Iteration 1486, loss = 0.00528721\n",
            "Iteration 1487, loss = 0.00529773\n",
            "Iteration 1488, loss = 0.00528525\n",
            "Iteration 1489, loss = 0.00528252\n",
            "Iteration 1490, loss = 0.00522691\n",
            "Iteration 1491, loss = 0.00522205\n",
            "Iteration 1492, loss = 0.00520463\n",
            "Iteration 1493, loss = 0.00520628\n",
            "Iteration 1494, loss = 0.00517997\n",
            "Iteration 1495, loss = 0.00518002\n",
            "Iteration 1496, loss = 0.00515258\n",
            "Iteration 1497, loss = 0.00515594\n",
            "Iteration 1498, loss = 0.00517044\n",
            "Iteration 1499, loss = 0.00512809\n",
            "Iteration 1500, loss = 0.00512689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(2, 2), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1500,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpUZuzCWVsO8",
        "outputId": "ffb821af-3b4a-48a3-f8e5-a6be742ddafe"
      },
      "source": [
        "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
        "previsoes"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfTYmNCkVyhY",
        "outputId": "bc449e49-09ff-45f4-a4ec-82570bafb6c4"
      },
      "source": [
        "y_credit_teste"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E_jbN7nV9SL",
        "outputId": "8f729b71-0f4d-438b-8f4f-cb621bea4557"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_credit_teste, previsoes)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTFkASVjWT1h"
      },
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2qT8OAzGWwGe",
        "outputId": "90747eec-6db6-4b90-bd29-a786092a963e"
      },
      "source": [
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste, y_credit_teste)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.46665474\n",
            "Iteration 2, loss = 0.45889319\n",
            "Iteration 3, loss = 0.45203136\n",
            "Iteration 4, loss = 0.44565090\n",
            "Iteration 5, loss = 0.43984775\n",
            "Iteration 6, loss = 0.43401277\n",
            "Iteration 7, loss = 0.42858976\n",
            "Iteration 8, loss = 0.42315478\n",
            "Iteration 9, loss = 0.41787009\n",
            "Iteration 10, loss = 0.41255609\n",
            "Iteration 11, loss = 0.40742777\n",
            "Iteration 12, loss = 0.40227279\n",
            "Iteration 13, loss = 0.39726471\n",
            "Iteration 14, loss = 0.39228938\n",
            "Iteration 15, loss = 0.38740420\n",
            "Iteration 16, loss = 0.38254720\n",
            "Iteration 17, loss = 0.37774916\n",
            "Iteration 18, loss = 0.37294518\n",
            "Iteration 19, loss = 0.36822852\n",
            "Iteration 20, loss = 0.36361172\n",
            "Iteration 21, loss = 0.35907853\n",
            "Iteration 22, loss = 0.35460784\n",
            "Iteration 23, loss = 0.35019945\n",
            "Iteration 24, loss = 0.34582941\n",
            "Iteration 25, loss = 0.34163657\n",
            "Iteration 26, loss = 0.33738603\n",
            "Iteration 27, loss = 0.33325535\n",
            "Iteration 28, loss = 0.32917248\n",
            "Iteration 29, loss = 0.32521356\n",
            "Iteration 30, loss = 0.32128890\n",
            "Iteration 31, loss = 0.31742070\n",
            "Iteration 32, loss = 0.31361331\n",
            "Iteration 33, loss = 0.30987224\n",
            "Iteration 34, loss = 0.30624037\n",
            "Iteration 35, loss = 0.30263309\n",
            "Iteration 36, loss = 0.29915850\n",
            "Iteration 37, loss = 0.29575604\n",
            "Iteration 38, loss = 0.29237638\n",
            "Iteration 39, loss = 0.28907408\n",
            "Iteration 40, loss = 0.28584151\n",
            "Iteration 41, loss = 0.28268429\n",
            "Iteration 42, loss = 0.27956474\n",
            "Iteration 43, loss = 0.27650827\n",
            "Iteration 44, loss = 0.27350325\n",
            "Iteration 45, loss = 0.27060578\n",
            "Iteration 46, loss = 0.26764828\n",
            "Iteration 47, loss = 0.26482225\n",
            "Iteration 48, loss = 0.26197276\n",
            "Iteration 49, loss = 0.25923646\n",
            "Iteration 50, loss = 0.25650680\n",
            "Iteration 51, loss = 0.25386051\n",
            "Iteration 52, loss = 0.25123402\n",
            "Iteration 53, loss = 0.24868595\n",
            "Iteration 54, loss = 0.24611683\n",
            "Iteration 55, loss = 0.24363643\n",
            "Iteration 56, loss = 0.24117224\n",
            "Iteration 57, loss = 0.23878131\n",
            "Iteration 58, loss = 0.23641660\n",
            "Iteration 59, loss = 0.23409761\n",
            "Iteration 60, loss = 0.23184752\n",
            "Iteration 61, loss = 0.22961841\n",
            "Iteration 62, loss = 0.22743758\n",
            "Iteration 63, loss = 0.22528981\n",
            "Iteration 64, loss = 0.22314199\n",
            "Iteration 65, loss = 0.22109880\n",
            "Iteration 66, loss = 0.21905898\n",
            "Iteration 67, loss = 0.21711351\n",
            "Iteration 68, loss = 0.21507325\n",
            "Iteration 69, loss = 0.21320241\n",
            "Iteration 70, loss = 0.21129983\n",
            "Iteration 71, loss = 0.20945677\n",
            "Iteration 72, loss = 0.20764571\n",
            "Iteration 73, loss = 0.20583181\n",
            "Iteration 74, loss = 0.20409596\n",
            "Iteration 75, loss = 0.20239048\n",
            "Iteration 76, loss = 0.20072738\n",
            "Iteration 77, loss = 0.19912976\n",
            "Iteration 78, loss = 0.19748263\n",
            "Iteration 79, loss = 0.19589493\n",
            "Iteration 80, loss = 0.19435940\n",
            "Iteration 81, loss = 0.19290946\n",
            "Iteration 82, loss = 0.19135087\n",
            "Iteration 83, loss = 0.18988870\n",
            "Iteration 84, loss = 0.18846568\n",
            "Iteration 85, loss = 0.18703540\n",
            "Iteration 86, loss = 0.18565613\n",
            "Iteration 87, loss = 0.18431084\n",
            "Iteration 88, loss = 0.18299740\n",
            "Iteration 89, loss = 0.18171743\n",
            "Iteration 90, loss = 0.18050072\n",
            "Iteration 91, loss = 0.17922660\n",
            "Iteration 92, loss = 0.17802902\n",
            "Iteration 93, loss = 0.17680167\n",
            "Iteration 94, loss = 0.17562149\n",
            "Iteration 95, loss = 0.17450647\n",
            "Iteration 96, loss = 0.17332747\n",
            "Iteration 97, loss = 0.17230698\n",
            "Iteration 98, loss = 0.17120430\n",
            "Iteration 99, loss = 0.17012281\n",
            "Iteration 100, loss = 0.16907671\n",
            "Iteration 101, loss = 0.16804913\n",
            "Iteration 102, loss = 0.16705678\n",
            "Iteration 103, loss = 0.16610220\n",
            "Iteration 104, loss = 0.16513441\n",
            "Iteration 105, loss = 0.16420361\n",
            "Iteration 106, loss = 0.16328349\n",
            "Iteration 107, loss = 0.16235311\n",
            "Iteration 108, loss = 0.16151188\n",
            "Iteration 109, loss = 0.16063403\n",
            "Iteration 110, loss = 0.15977738\n",
            "Iteration 111, loss = 0.15892456\n",
            "Iteration 112, loss = 0.15813441\n",
            "Iteration 113, loss = 0.15737429\n",
            "Iteration 114, loss = 0.15657909\n",
            "Iteration 115, loss = 0.15582139\n",
            "Iteration 116, loss = 0.15505220\n",
            "Iteration 117, loss = 0.15431345\n",
            "Iteration 118, loss = 0.15363756\n",
            "Iteration 119, loss = 0.15288321\n",
            "Iteration 120, loss = 0.15218187\n",
            "Iteration 121, loss = 0.15152626\n",
            "Iteration 122, loss = 0.15085257\n",
            "Iteration 123, loss = 0.15018894\n",
            "Iteration 124, loss = 0.14955138\n",
            "Iteration 125, loss = 0.14894178\n",
            "Iteration 126, loss = 0.14834435\n",
            "Iteration 127, loss = 0.14776041\n",
            "Iteration 128, loss = 0.14713371\n",
            "Iteration 129, loss = 0.14656030\n",
            "Iteration 130, loss = 0.14598658\n",
            "Iteration 131, loss = 0.14545596\n",
            "Iteration 132, loss = 0.14487961\n",
            "Iteration 133, loss = 0.14437167\n",
            "Iteration 134, loss = 0.14382513\n",
            "Iteration 135, loss = 0.14332186\n",
            "Iteration 136, loss = 0.14280826\n",
            "Iteration 137, loss = 0.14231060\n",
            "Iteration 138, loss = 0.14181096\n",
            "Iteration 139, loss = 0.14133416\n",
            "Iteration 140, loss = 0.14086539\n",
            "Iteration 141, loss = 0.14040155\n",
            "Iteration 142, loss = 0.13994672\n",
            "Iteration 143, loss = 0.13949132\n",
            "Iteration 144, loss = 0.13905548\n",
            "Iteration 145, loss = 0.13867895\n",
            "Iteration 146, loss = 0.13821346\n",
            "Iteration 147, loss = 0.13776389\n",
            "Iteration 148, loss = 0.13735796\n",
            "Iteration 149, loss = 0.13697318\n",
            "Iteration 150, loss = 0.13658697\n",
            "Iteration 151, loss = 0.13624636\n",
            "Iteration 152, loss = 0.13585627\n",
            "Iteration 153, loss = 0.13553604\n",
            "Iteration 154, loss = 0.13509607\n",
            "Iteration 155, loss = 0.13480419\n",
            "Iteration 156, loss = 0.13439992\n",
            "Iteration 157, loss = 0.13404688\n",
            "Iteration 158, loss = 0.13377357\n",
            "Iteration 159, loss = 0.13338373\n",
            "Iteration 160, loss = 0.13304581\n",
            "Iteration 161, loss = 0.13275182\n",
            "Iteration 162, loss = 0.13246159\n",
            "Iteration 163, loss = 0.13212919\n",
            "Iteration 164, loss = 0.13187004\n",
            "Iteration 165, loss = 0.13155728\n",
            "Iteration 166, loss = 0.13124018\n",
            "Iteration 167, loss = 0.13098387\n",
            "Iteration 168, loss = 0.13078745\n",
            "Iteration 169, loss = 0.13053842\n",
            "Iteration 170, loss = 0.13014042\n",
            "Iteration 171, loss = 0.12988726\n",
            "Iteration 172, loss = 0.12962169\n",
            "Iteration 173, loss = 0.12939560\n",
            "Iteration 174, loss = 0.12918437\n",
            "Iteration 175, loss = 0.12894182\n",
            "Iteration 176, loss = 0.12863437\n",
            "Iteration 177, loss = 0.12841474\n",
            "Iteration 178, loss = 0.12820427\n",
            "Iteration 179, loss = 0.12793434\n",
            "Iteration 180, loss = 0.12771115\n",
            "Iteration 181, loss = 0.12754382\n",
            "Iteration 182, loss = 0.12738974\n",
            "Iteration 183, loss = 0.12716500\n",
            "Iteration 184, loss = 0.12689080\n",
            "Iteration 185, loss = 0.12668437\n",
            "Iteration 186, loss = 0.12647787\n",
            "Iteration 187, loss = 0.12630241\n",
            "Iteration 188, loss = 0.12609183\n",
            "Iteration 189, loss = 0.12596114\n",
            "Iteration 190, loss = 0.12574467\n",
            "Iteration 191, loss = 0.12550924\n",
            "Iteration 192, loss = 0.12534046\n",
            "Iteration 193, loss = 0.12512709\n",
            "Iteration 194, loss = 0.12495631\n",
            "Iteration 195, loss = 0.12478252\n",
            "Iteration 196, loss = 0.12465964\n",
            "Iteration 197, loss = 0.12445887\n",
            "Iteration 198, loss = 0.12440100\n",
            "Iteration 199, loss = 0.12415852\n",
            "Iteration 200, loss = 0.12399712\n",
            "Iteration 201, loss = 0.12394174\n",
            "Iteration 202, loss = 0.12372938\n",
            "Iteration 203, loss = 0.12356025\n",
            "Iteration 204, loss = 0.12339383\n",
            "Iteration 205, loss = 0.12325787\n",
            "Iteration 206, loss = 0.12311101\n",
            "Iteration 207, loss = 0.12297218\n",
            "Iteration 208, loss = 0.12283832\n",
            "Iteration 209, loss = 0.12268044\n",
            "Iteration 210, loss = 0.12258885\n",
            "Iteration 211, loss = 0.12245949\n",
            "Iteration 212, loss = 0.12232778\n",
            "Iteration 213, loss = 0.12220902\n",
            "Iteration 214, loss = 0.12214525\n",
            "Iteration 215, loss = 0.12198721\n",
            "Iteration 216, loss = 0.12186194\n",
            "Iteration 217, loss = 0.12173382\n",
            "Iteration 218, loss = 0.12159910\n",
            "Iteration 219, loss = 0.12149349\n",
            "Iteration 220, loss = 0.12138435\n",
            "Iteration 221, loss = 0.12136659\n",
            "Iteration 222, loss = 0.12115599\n",
            "Iteration 223, loss = 0.12103362\n",
            "Iteration 224, loss = 0.12092712\n",
            "Iteration 225, loss = 0.12090453\n",
            "Iteration 226, loss = 0.12076288\n",
            "Iteration 227, loss = 0.12062220\n",
            "Iteration 228, loss = 0.12051539\n",
            "Iteration 229, loss = 0.12047802\n",
            "Iteration 230, loss = 0.12036836\n",
            "Iteration 231, loss = 0.12026445\n",
            "Iteration 232, loss = 0.12015875\n",
            "Iteration 233, loss = 0.12014488\n",
            "Iteration 234, loss = 0.12003035\n",
            "Iteration 235, loss = 0.11999693\n",
            "Iteration 236, loss = 0.11982964\n",
            "Iteration 237, loss = 0.11975259\n",
            "Iteration 238, loss = 0.11970356\n",
            "Iteration 239, loss = 0.11958850\n",
            "Iteration 240, loss = 0.11951648\n",
            "Iteration 241, loss = 0.11943210\n",
            "Iteration 242, loss = 0.11936169\n",
            "Iteration 243, loss = 0.11930145\n",
            "Iteration 244, loss = 0.11918154\n",
            "Iteration 245, loss = 0.11918003\n",
            "Iteration 246, loss = 0.11905145\n",
            "Iteration 247, loss = 0.11897934\n",
            "Iteration 248, loss = 0.11904071\n",
            "Iteration 249, loss = 0.11891043\n",
            "Iteration 250, loss = 0.11874368\n",
            "Iteration 251, loss = 0.11872198\n",
            "Iteration 252, loss = 0.11860719\n",
            "Iteration 253, loss = 0.11858085\n",
            "Iteration 254, loss = 0.11853461\n",
            "Iteration 255, loss = 0.11845728\n",
            "Iteration 256, loss = 0.11837635\n",
            "Iteration 257, loss = 0.11834169\n",
            "Iteration 258, loss = 0.11828219\n",
            "Iteration 259, loss = 0.11825565\n",
            "Iteration 260, loss = 0.11815238\n",
            "Iteration 261, loss = 0.11812021\n",
            "Iteration 262, loss = 0.11810865\n",
            "Iteration 263, loss = 0.11813838\n",
            "Iteration 264, loss = 0.11795877\n",
            "Iteration 265, loss = 0.11788692\n",
            "Iteration 266, loss = 0.11781119\n",
            "Iteration 267, loss = 0.11782013\n",
            "Iteration 268, loss = 0.11772143\n",
            "Iteration 269, loss = 0.11766085\n",
            "Iteration 270, loss = 0.11759616\n",
            "Iteration 271, loss = 0.11758026\n",
            "Iteration 272, loss = 0.11751522\n",
            "Iteration 273, loss = 0.11746486\n",
            "Iteration 274, loss = 0.11743504\n",
            "Iteration 275, loss = 0.11736949\n",
            "Iteration 276, loss = 0.11733614\n",
            "Iteration 277, loss = 0.11729654\n",
            "Iteration 278, loss = 0.11728418\n",
            "Iteration 279, loss = 0.11723626\n",
            "Iteration 280, loss = 0.11715558\n",
            "Iteration 281, loss = 0.11715748\n",
            "Iteration 282, loss = 0.11714401\n",
            "Iteration 283, loss = 0.11704375\n",
            "Iteration 284, loss = 0.11700117\n",
            "Iteration 285, loss = 0.11696138\n",
            "Iteration 286, loss = 0.11697724\n",
            "Iteration 287, loss = 0.11693146\n",
            "Iteration 288, loss = 0.11683218\n",
            "Iteration 289, loss = 0.11681946\n",
            "Iteration 290, loss = 0.11678819\n",
            "Iteration 291, loss = 0.11675067\n",
            "Iteration 292, loss = 0.11669514\n",
            "Iteration 293, loss = 0.11665756\n",
            "Iteration 294, loss = 0.11661887\n",
            "Iteration 295, loss = 0.11657425\n",
            "Iteration 296, loss = 0.11655122\n",
            "Iteration 297, loss = 0.11655699\n",
            "Iteration 298, loss = 0.11647831\n",
            "Iteration 299, loss = 0.11647712\n",
            "Iteration 300, loss = 0.11638897\n",
            "Iteration 301, loss = 0.11636509\n",
            "Iteration 302, loss = 0.11634712\n",
            "Iteration 303, loss = 0.11631339\n",
            "Iteration 304, loss = 0.11629632\n",
            "Iteration 305, loss = 0.11627460\n",
            "Iteration 306, loss = 0.11632476\n",
            "Iteration 307, loss = 0.11618116\n",
            "Iteration 308, loss = 0.11615469\n",
            "Iteration 309, loss = 0.11612631\n",
            "Iteration 310, loss = 0.11609199\n",
            "Iteration 311, loss = 0.11610863\n",
            "Iteration 312, loss = 0.11604839\n",
            "Iteration 313, loss = 0.11600902\n",
            "Iteration 314, loss = 0.11599649\n",
            "Iteration 315, loss = 0.11595692\n",
            "Iteration 316, loss = 0.11595831\n",
            "Iteration 317, loss = 0.11589819\n",
            "Iteration 318, loss = 0.11585652\n",
            "Iteration 319, loss = 0.11584039\n",
            "Iteration 320, loss = 0.11582049\n",
            "Iteration 321, loss = 0.11581494\n",
            "Iteration 322, loss = 0.11581309\n",
            "Iteration 323, loss = 0.11573419\n",
            "Iteration 324, loss = 0.11579654\n",
            "Iteration 325, loss = 0.11572465\n",
            "Iteration 326, loss = 0.11567071\n",
            "Iteration 327, loss = 0.11564889\n",
            "Iteration 328, loss = 0.11566999\n",
            "Iteration 329, loss = 0.11562521\n",
            "Iteration 330, loss = 0.11556133\n",
            "Iteration 331, loss = 0.11555619\n",
            "Iteration 332, loss = 0.11559037\n",
            "Iteration 333, loss = 0.11550166\n",
            "Iteration 334, loss = 0.11545707\n",
            "Iteration 335, loss = 0.11546906\n",
            "Iteration 336, loss = 0.11541443\n",
            "Iteration 337, loss = 0.11538521\n",
            "Iteration 338, loss = 0.11536430\n",
            "Iteration 339, loss = 0.11538654\n",
            "Iteration 340, loss = 0.11539614\n",
            "Iteration 341, loss = 0.11531957\n",
            "Iteration 342, loss = 0.11535449\n",
            "Iteration 343, loss = 0.11529476\n",
            "Iteration 344, loss = 0.11537046\n",
            "Iteration 345, loss = 0.11525899\n",
            "Iteration 346, loss = 0.11521745\n",
            "Iteration 347, loss = 0.11520365\n",
            "Iteration 348, loss = 0.11516100\n",
            "Iteration 349, loss = 0.11514580\n",
            "Iteration 350, loss = 0.11516736\n",
            "Iteration 351, loss = 0.11514581\n",
            "Iteration 352, loss = 0.11512002\n",
            "Iteration 353, loss = 0.11519676\n",
            "Iteration 354, loss = 0.11510678\n",
            "Iteration 355, loss = 0.11512075\n",
            "Iteration 356, loss = 0.11501021\n",
            "Iteration 357, loss = 0.11502576\n",
            "Iteration 358, loss = 0.11499448\n",
            "Iteration 359, loss = 0.11498088\n",
            "Iteration 360, loss = 0.11493011\n",
            "Iteration 361, loss = 0.11498043\n",
            "Iteration 362, loss = 0.11493196\n",
            "Iteration 363, loss = 0.11491431\n",
            "Iteration 364, loss = 0.11489674\n",
            "Iteration 365, loss = 0.11488285\n",
            "Iteration 366, loss = 0.11488741\n",
            "Iteration 367, loss = 0.11480898\n",
            "Iteration 368, loss = 0.11481048\n",
            "Iteration 369, loss = 0.11479977\n",
            "Iteration 370, loss = 0.11479658\n",
            "Iteration 371, loss = 0.11476859\n",
            "Iteration 372, loss = 0.11475424\n",
            "Iteration 373, loss = 0.11479055\n",
            "Iteration 374, loss = 0.11474555\n",
            "Iteration 375, loss = 0.11470525\n",
            "Iteration 376, loss = 0.11470713\n",
            "Iteration 377, loss = 0.11467600\n",
            "Iteration 378, loss = 0.11484354\n",
            "Iteration 379, loss = 0.11468697\n",
            "Iteration 380, loss = 0.11467503\n",
            "Iteration 381, loss = 0.11465137\n",
            "Iteration 382, loss = 0.11459015\n",
            "Iteration 383, loss = 0.11458495\n",
            "Iteration 384, loss = 0.11459341\n",
            "Iteration 385, loss = 0.11460266\n",
            "Iteration 386, loss = 0.11454617\n",
            "Iteration 387, loss = 0.11454687\n",
            "Iteration 388, loss = 0.11453375\n",
            "Iteration 389, loss = 0.11453494\n",
            "Iteration 390, loss = 0.11452822\n",
            "Iteration 391, loss = 0.11461047\n",
            "Iteration 392, loss = 0.11451829\n",
            "Iteration 393, loss = 0.11454587\n",
            "Iteration 394, loss = 0.11444043\n",
            "Iteration 395, loss = 0.11450407\n",
            "Iteration 396, loss = 0.11445436\n",
            "Iteration 397, loss = 0.11445612\n",
            "Iteration 398, loss = 0.11444055\n",
            "Iteration 399, loss = 0.11436056\n",
            "Iteration 400, loss = 0.11437607\n",
            "Iteration 401, loss = 0.11444527\n",
            "Iteration 402, loss = 0.11433795\n",
            "Iteration 403, loss = 0.11448578\n",
            "Iteration 404, loss = 0.11433359\n",
            "Iteration 405, loss = 0.11434768\n",
            "Iteration 406, loss = 0.11437569\n",
            "Iteration 407, loss = 0.11428787\n",
            "Iteration 408, loss = 0.11430407\n",
            "Iteration 409, loss = 0.11426453\n",
            "Iteration 410, loss = 0.11428650\n",
            "Iteration 411, loss = 0.11419863\n",
            "Iteration 412, loss = 0.11420883\n",
            "Iteration 413, loss = 0.11421054\n",
            "Iteration 414, loss = 0.11426130\n",
            "Iteration 415, loss = 0.11421662\n",
            "Iteration 416, loss = 0.11422206\n",
            "Iteration 417, loss = 0.11419968\n",
            "Iteration 418, loss = 0.11414220\n",
            "Iteration 419, loss = 0.11412220\n",
            "Iteration 420, loss = 0.11416215\n",
            "Iteration 421, loss = 0.11417073\n",
            "Iteration 422, loss = 0.11414310\n",
            "Iteration 423, loss = 0.11415489\n",
            "Iteration 424, loss = 0.11406770\n",
            "Iteration 425, loss = 0.11406706\n",
            "Iteration 426, loss = 0.11412498\n",
            "Iteration 427, loss = 0.11404826\n",
            "Iteration 428, loss = 0.11404062\n",
            "Iteration 429, loss = 0.11404440\n",
            "Iteration 430, loss = 0.11401437\n",
            "Iteration 431, loss = 0.11402005\n",
            "Iteration 432, loss = 0.11400422\n",
            "Iteration 433, loss = 0.11396999\n",
            "Iteration 434, loss = 0.11409556\n",
            "Iteration 435, loss = 0.11399478\n",
            "Iteration 436, loss = 0.11393113\n",
            "Iteration 437, loss = 0.11393920\n",
            "Iteration 438, loss = 0.11392475\n",
            "Iteration 439, loss = 0.11388972\n",
            "Iteration 440, loss = 0.11388449\n",
            "Iteration 441, loss = 0.11393216\n",
            "Iteration 442, loss = 0.11390389\n",
            "Iteration 443, loss = 0.11388187\n",
            "Iteration 444, loss = 0.11384973\n",
            "Iteration 445, loss = 0.11390418\n",
            "Iteration 446, loss = 0.11388410\n",
            "Iteration 447, loss = 0.11379882\n",
            "Iteration 448, loss = 0.11380673\n",
            "Iteration 449, loss = 0.11379724\n",
            "Iteration 450, loss = 0.11378738\n",
            "Iteration 451, loss = 0.11390216\n",
            "Iteration 452, loss = 0.11375394\n",
            "Iteration 453, loss = 0.11376793\n",
            "Iteration 454, loss = 0.11385980\n",
            "Iteration 455, loss = 0.11378213\n",
            "Iteration 456, loss = 0.11377935\n",
            "Iteration 457, loss = 0.11371418\n",
            "Iteration 458, loss = 0.11371950\n",
            "Iteration 459, loss = 0.11373363\n",
            "Iteration 460, loss = 0.11379604\n",
            "Iteration 461, loss = 0.11371116\n",
            "Iteration 462, loss = 0.11373282\n",
            "Iteration 463, loss = 0.11367941\n",
            "Iteration 464, loss = 0.11367594\n",
            "Iteration 465, loss = 0.11365410\n",
            "Iteration 466, loss = 0.11364355\n",
            "Iteration 467, loss = 0.11371015\n",
            "Iteration 468, loss = 0.11362486\n",
            "Iteration 469, loss = 0.11365648\n",
            "Iteration 470, loss = 0.11363295\n",
            "Iteration 471, loss = 0.11359825\n",
            "Iteration 472, loss = 0.11361230\n",
            "Iteration 473, loss = 0.11358476\n",
            "Iteration 474, loss = 0.11363536\n",
            "Iteration 475, loss = 0.11359692\n",
            "Iteration 476, loss = 0.11358532\n",
            "Iteration 477, loss = 0.11357557\n",
            "Iteration 478, loss = 0.11356666\n",
            "Iteration 479, loss = 0.11353850\n",
            "Iteration 480, loss = 0.11354057\n",
            "Iteration 481, loss = 0.11356650\n",
            "Iteration 482, loss = 0.11353237\n",
            "Iteration 483, loss = 0.11356587\n",
            "Iteration 484, loss = 0.11355740\n",
            "Iteration 485, loss = 0.11353589\n",
            "Iteration 486, loss = 0.11350048\n",
            "Iteration 487, loss = 0.11348158\n",
            "Iteration 488, loss = 0.11358459\n",
            "Iteration 489, loss = 0.11349392\n",
            "Iteration 490, loss = 0.11347815\n",
            "Iteration 491, loss = 0.11346613\n",
            "Iteration 492, loss = 0.11350700\n",
            "Iteration 493, loss = 0.11349948\n",
            "Iteration 494, loss = 0.11344302\n",
            "Iteration 495, loss = 0.11342649\n",
            "Iteration 496, loss = 0.11347616\n",
            "Iteration 497, loss = 0.11341252\n",
            "Iteration 498, loss = 0.11339173\n",
            "Iteration 499, loss = 0.11342419\n",
            "Iteration 500, loss = 0.11350326\n",
            "Iteration 501, loss = 0.11345996\n",
            "Iteration 502, loss = 0.11337950\n",
            "Iteration 503, loss = 0.11337376\n",
            "Iteration 504, loss = 0.11340369\n",
            "Iteration 505, loss = 0.11333006\n",
            "Iteration 506, loss = 0.11334365\n",
            "Iteration 507, loss = 0.11336535\n",
            "Iteration 508, loss = 0.11337228\n",
            "Iteration 509, loss = 0.11331024\n",
            "Iteration 510, loss = 0.11329358\n",
            "Iteration 511, loss = 0.11328945\n",
            "Iteration 512, loss = 0.11327990\n",
            "Iteration 513, loss = 0.11326654\n",
            "Iteration 514, loss = 0.11326335\n",
            "Iteration 515, loss = 0.11333464\n",
            "Iteration 516, loss = 0.11333157\n",
            "Iteration 517, loss = 0.11328638\n",
            "Iteration 518, loss = 0.11327133\n",
            "Iteration 519, loss = 0.11324710\n",
            "Iteration 520, loss = 0.11324689\n",
            "Iteration 521, loss = 0.11322399\n",
            "Iteration 522, loss = 0.11327539\n",
            "Iteration 523, loss = 0.11316962\n",
            "Iteration 524, loss = 0.11317215\n",
            "Iteration 525, loss = 0.11323315\n",
            "Iteration 526, loss = 0.11315449\n",
            "Iteration 527, loss = 0.11318353\n",
            "Iteration 528, loss = 0.11319220\n",
            "Iteration 529, loss = 0.11312239\n",
            "Iteration 530, loss = 0.11313535\n",
            "Iteration 531, loss = 0.11311484\n",
            "Iteration 532, loss = 0.11309797\n",
            "Iteration 533, loss = 0.11310447\n",
            "Iteration 534, loss = 0.11318023\n",
            "Iteration 535, loss = 0.11309324\n",
            "Iteration 536, loss = 0.11310675\n",
            "Iteration 537, loss = 0.11305289\n",
            "Iteration 538, loss = 0.11304548\n",
            "Iteration 539, loss = 0.11312049\n",
            "Iteration 540, loss = 0.11308684\n",
            "Iteration 541, loss = 0.11306939\n",
            "Iteration 542, loss = 0.11303673\n",
            "Iteration 543, loss = 0.11304594\n",
            "Iteration 544, loss = 0.11298553\n",
            "Iteration 545, loss = 0.11304825\n",
            "Iteration 546, loss = 0.11294454\n",
            "Iteration 547, loss = 0.11292620\n",
            "Iteration 548, loss = 0.11292375\n",
            "Iteration 549, loss = 0.11297489\n",
            "Iteration 550, loss = 0.11308983\n",
            "Iteration 551, loss = 0.11281132\n",
            "Iteration 552, loss = 0.11294776\n",
            "Iteration 553, loss = 0.11291693\n",
            "Iteration 554, loss = 0.11288077\n",
            "Iteration 555, loss = 0.11289078\n",
            "Iteration 556, loss = 0.11287701\n",
            "Iteration 557, loss = 0.11281328\n",
            "Iteration 558, loss = 0.11279549\n",
            "Iteration 559, loss = 0.11279899\n",
            "Iteration 560, loss = 0.11283891\n",
            "Iteration 561, loss = 0.11277250\n",
            "Iteration 562, loss = 0.11276180\n",
            "Iteration 563, loss = 0.11275018\n",
            "Iteration 564, loss = 0.11269977\n",
            "Iteration 565, loss = 0.11274757\n",
            "Iteration 566, loss = 0.11268252\n",
            "Iteration 567, loss = 0.11265216\n",
            "Iteration 568, loss = 0.11260790\n",
            "Iteration 569, loss = 0.11268233\n",
            "Iteration 570, loss = 0.11272319\n",
            "Iteration 571, loss = 0.11259305\n",
            "Iteration 572, loss = 0.11251550\n",
            "Iteration 573, loss = 0.11258362\n",
            "Iteration 574, loss = 0.11247153\n",
            "Iteration 575, loss = 0.11244133\n",
            "Iteration 576, loss = 0.11242441\n",
            "Iteration 577, loss = 0.11240268\n",
            "Iteration 578, loss = 0.11237033\n",
            "Iteration 579, loss = 0.11240640\n",
            "Iteration 580, loss = 0.11231263\n",
            "Iteration 581, loss = 0.11228284\n",
            "Iteration 582, loss = 0.11226970\n",
            "Iteration 583, loss = 0.11221748\n",
            "Iteration 584, loss = 0.11226182\n",
            "Iteration 585, loss = 0.11212218\n",
            "Iteration 586, loss = 0.11206694\n",
            "Iteration 587, loss = 0.11200575\n",
            "Iteration 588, loss = 0.11197566\n",
            "Iteration 589, loss = 0.11190167\n",
            "Iteration 590, loss = 0.11186339\n",
            "Iteration 591, loss = 0.11182152\n",
            "Iteration 592, loss = 0.11179145\n",
            "Iteration 593, loss = 0.11171353\n",
            "Iteration 594, loss = 0.11167944\n",
            "Iteration 595, loss = 0.11160247\n",
            "Iteration 596, loss = 0.11153173\n",
            "Iteration 597, loss = 0.11154346\n",
            "Iteration 598, loss = 0.11138519\n",
            "Iteration 599, loss = 0.11120780\n",
            "Iteration 600, loss = 0.11095008\n",
            "Iteration 601, loss = 0.11078464\n",
            "Iteration 602, loss = 0.11042049\n",
            "Iteration 603, loss = 0.11010315\n",
            "Iteration 604, loss = 0.10989388\n",
            "Iteration 605, loss = 0.10935489\n",
            "Iteration 606, loss = 0.10878118\n",
            "Iteration 607, loss = 0.10829213\n",
            "Iteration 608, loss = 0.10763921\n",
            "Iteration 609, loss = 0.10697695\n",
            "Iteration 610, loss = 0.10617480\n",
            "Iteration 611, loss = 0.10547645\n",
            "Iteration 612, loss = 0.10479955\n",
            "Iteration 613, loss = 0.10402060\n",
            "Iteration 614, loss = 0.10314823\n",
            "Iteration 615, loss = 0.10210560\n",
            "Iteration 616, loss = 0.10087768\n",
            "Iteration 617, loss = 0.09954221\n",
            "Iteration 618, loss = 0.09802657\n",
            "Iteration 619, loss = 0.09611170\n",
            "Iteration 620, loss = 0.09385983\n",
            "Iteration 621, loss = 0.09180297\n",
            "Iteration 622, loss = 0.08965646\n",
            "Iteration 623, loss = 0.08776267\n",
            "Iteration 624, loss = 0.08625350\n",
            "Iteration 625, loss = 0.08498125\n",
            "Iteration 626, loss = 0.08391287\n",
            "Iteration 627, loss = 0.08288548\n",
            "Iteration 628, loss = 0.08219296\n",
            "Iteration 629, loss = 0.08146584\n",
            "Iteration 630, loss = 0.08076636\n",
            "Iteration 631, loss = 0.08008817\n",
            "Iteration 632, loss = 0.07948390\n",
            "Iteration 633, loss = 0.07897939\n",
            "Iteration 634, loss = 0.07829683\n",
            "Iteration 635, loss = 0.07778122\n",
            "Iteration 636, loss = 0.07717049\n",
            "Iteration 637, loss = 0.07663484\n",
            "Iteration 638, loss = 0.07614765\n",
            "Iteration 639, loss = 0.07556950\n",
            "Iteration 640, loss = 0.07507471\n",
            "Iteration 641, loss = 0.07458221\n",
            "Iteration 642, loss = 0.07408872\n",
            "Iteration 643, loss = 0.07358466\n",
            "Iteration 644, loss = 0.07311132\n",
            "Iteration 645, loss = 0.07282181\n",
            "Iteration 646, loss = 0.07221784\n",
            "Iteration 647, loss = 0.07183014\n",
            "Iteration 648, loss = 0.07128647\n",
            "Iteration 649, loss = 0.07086114\n",
            "Iteration 650, loss = 0.07045735\n",
            "Iteration 651, loss = 0.07005119\n",
            "Iteration 652, loss = 0.06957089\n",
            "Iteration 653, loss = 0.06918831\n",
            "Iteration 654, loss = 0.06876004\n",
            "Iteration 655, loss = 0.06829960\n",
            "Iteration 656, loss = 0.06792096\n",
            "Iteration 657, loss = 0.06752562\n",
            "Iteration 658, loss = 0.06715150\n",
            "Iteration 659, loss = 0.06673903\n",
            "Iteration 660, loss = 0.06632407\n",
            "Iteration 661, loss = 0.06597746\n",
            "Iteration 662, loss = 0.06554853\n",
            "Iteration 663, loss = 0.06515029\n",
            "Iteration 664, loss = 0.06487404\n",
            "Iteration 665, loss = 0.06446208\n",
            "Iteration 666, loss = 0.06406805\n",
            "Iteration 667, loss = 0.06371715\n",
            "Iteration 668, loss = 0.06334930\n",
            "Iteration 669, loss = 0.06300083\n",
            "Iteration 670, loss = 0.06271765\n",
            "Iteration 671, loss = 0.06230057\n",
            "Iteration 672, loss = 0.06193998\n",
            "Iteration 673, loss = 0.06161282\n",
            "Iteration 674, loss = 0.06126658\n",
            "Iteration 675, loss = 0.06093091\n",
            "Iteration 676, loss = 0.06064399\n",
            "Iteration 677, loss = 0.06029748\n",
            "Iteration 678, loss = 0.05993065\n",
            "Iteration 679, loss = 0.05963910\n",
            "Iteration 680, loss = 0.05927548\n",
            "Iteration 681, loss = 0.05900019\n",
            "Iteration 682, loss = 0.05865967\n",
            "Iteration 683, loss = 0.05835790\n",
            "Iteration 684, loss = 0.05803169\n",
            "Iteration 685, loss = 0.05772428\n",
            "Iteration 686, loss = 0.05739303\n",
            "Iteration 687, loss = 0.05721913\n",
            "Iteration 688, loss = 0.05680886\n",
            "Iteration 689, loss = 0.05649886\n",
            "Iteration 690, loss = 0.05617325\n",
            "Iteration 691, loss = 0.05590536\n",
            "Iteration 692, loss = 0.05559679\n",
            "Iteration 693, loss = 0.05535590\n",
            "Iteration 694, loss = 0.05505533\n",
            "Iteration 695, loss = 0.05482503\n",
            "Iteration 696, loss = 0.05447233\n",
            "Iteration 697, loss = 0.05414723\n",
            "Iteration 698, loss = 0.05388257\n",
            "Iteration 699, loss = 0.05360009\n",
            "Iteration 700, loss = 0.05332668\n",
            "Iteration 701, loss = 0.05302167\n",
            "Iteration 702, loss = 0.05274412\n",
            "Iteration 703, loss = 0.05249846\n",
            "Iteration 704, loss = 0.05228424\n",
            "Iteration 705, loss = 0.05201155\n",
            "Iteration 706, loss = 0.05165842\n",
            "Iteration 707, loss = 0.05143084\n",
            "Iteration 708, loss = 0.05118399\n",
            "Iteration 709, loss = 0.05091596\n",
            "Iteration 710, loss = 0.05065587\n",
            "Iteration 711, loss = 0.05037606\n",
            "Iteration 712, loss = 0.05012213\n",
            "Iteration 713, loss = 0.04987349\n",
            "Iteration 714, loss = 0.04963562\n",
            "Iteration 715, loss = 0.04939919\n",
            "Iteration 716, loss = 0.04910859\n",
            "Iteration 717, loss = 0.04887651\n",
            "Iteration 718, loss = 0.04862201\n",
            "Iteration 719, loss = 0.04840512\n",
            "Iteration 720, loss = 0.04816029\n",
            "Iteration 721, loss = 0.04791276\n",
            "Iteration 722, loss = 0.04770256\n",
            "Iteration 723, loss = 0.04744380\n",
            "Iteration 724, loss = 0.04722145\n",
            "Iteration 725, loss = 0.04701191\n",
            "Iteration 726, loss = 0.04674817\n",
            "Iteration 727, loss = 0.04654734\n",
            "Iteration 728, loss = 0.04636693\n",
            "Iteration 729, loss = 0.04622311\n",
            "Iteration 730, loss = 0.04589526\n",
            "Iteration 731, loss = 0.04565587\n",
            "Iteration 732, loss = 0.04547074\n",
            "Iteration 733, loss = 0.04519348\n",
            "Iteration 734, loss = 0.04510610\n",
            "Iteration 735, loss = 0.04483442\n",
            "Iteration 736, loss = 0.04460239\n",
            "Iteration 737, loss = 0.04437544\n",
            "Iteration 738, loss = 0.04420173\n",
            "Iteration 739, loss = 0.04397507\n",
            "Iteration 740, loss = 0.04377066\n",
            "Iteration 741, loss = 0.04357472\n",
            "Iteration 742, loss = 0.04334006\n",
            "Iteration 743, loss = 0.04322906\n",
            "Iteration 744, loss = 0.04299369\n",
            "Iteration 745, loss = 0.04277243\n",
            "Iteration 746, loss = 0.04258557\n",
            "Iteration 747, loss = 0.04234877\n",
            "Iteration 748, loss = 0.04215796\n",
            "Iteration 749, loss = 0.04195278\n",
            "Iteration 750, loss = 0.04176155\n",
            "Iteration 751, loss = 0.04157174\n",
            "Iteration 752, loss = 0.04139358\n",
            "Iteration 753, loss = 0.04130093\n",
            "Iteration 754, loss = 0.04101845\n",
            "Iteration 755, loss = 0.04084361\n",
            "Iteration 756, loss = 0.04069274\n",
            "Iteration 757, loss = 0.04048798\n",
            "Iteration 758, loss = 0.04028233\n",
            "Iteration 759, loss = 0.04014144\n",
            "Iteration 760, loss = 0.03990431\n",
            "Iteration 761, loss = 0.03976320\n",
            "Iteration 762, loss = 0.03956357\n",
            "Iteration 763, loss = 0.03939921\n",
            "Iteration 764, loss = 0.03920790\n",
            "Iteration 765, loss = 0.03906398\n",
            "Iteration 766, loss = 0.03887761\n",
            "Iteration 767, loss = 0.03869053\n",
            "Iteration 768, loss = 0.03859547\n",
            "Iteration 769, loss = 0.03842547\n",
            "Iteration 770, loss = 0.03817359\n",
            "Iteration 771, loss = 0.03802453\n",
            "Iteration 772, loss = 0.03783362\n",
            "Iteration 773, loss = 0.03767339\n",
            "Iteration 774, loss = 0.03754392\n",
            "Iteration 775, loss = 0.03738505\n",
            "Iteration 776, loss = 0.03724006\n",
            "Iteration 777, loss = 0.03715281\n",
            "Iteration 778, loss = 0.03685170\n",
            "Iteration 779, loss = 0.03680366\n",
            "Iteration 780, loss = 0.03657989\n",
            "Iteration 781, loss = 0.03640836\n",
            "Iteration 782, loss = 0.03630039\n",
            "Iteration 783, loss = 0.03611906\n",
            "Iteration 784, loss = 0.03595882\n",
            "Iteration 785, loss = 0.03585558\n",
            "Iteration 786, loss = 0.03565542\n",
            "Iteration 787, loss = 0.03558538\n",
            "Iteration 788, loss = 0.03542788\n",
            "Iteration 789, loss = 0.03525270\n",
            "Iteration 790, loss = 0.03513375\n",
            "Iteration 791, loss = 0.03500962\n",
            "Iteration 792, loss = 0.03481645\n",
            "Iteration 793, loss = 0.03464596\n",
            "Iteration 794, loss = 0.03452959\n",
            "Iteration 795, loss = 0.03435473\n",
            "Iteration 796, loss = 0.03425193\n",
            "Iteration 797, loss = 0.03410611\n",
            "Iteration 798, loss = 0.03398772\n",
            "Iteration 799, loss = 0.03384960\n",
            "Iteration 800, loss = 0.03368960\n",
            "Iteration 801, loss = 0.03356102\n",
            "Iteration 802, loss = 0.03347489\n",
            "Iteration 803, loss = 0.03329973\n",
            "Iteration 804, loss = 0.03320526\n",
            "Iteration 805, loss = 0.03305836\n",
            "Iteration 806, loss = 0.03296728\n",
            "Iteration 807, loss = 0.03279307\n",
            "Iteration 808, loss = 0.03264472\n",
            "Iteration 809, loss = 0.03252812\n",
            "Iteration 810, loss = 0.03243201\n",
            "Iteration 811, loss = 0.03227794\n",
            "Iteration 812, loss = 0.03217721\n",
            "Iteration 813, loss = 0.03205460\n",
            "Iteration 814, loss = 0.03193577\n",
            "Iteration 815, loss = 0.03178651\n",
            "Iteration 816, loss = 0.03171937\n",
            "Iteration 817, loss = 0.03157318\n",
            "Iteration 818, loss = 0.03144906\n",
            "Iteration 819, loss = 0.03133044\n",
            "Iteration 820, loss = 0.03124101\n",
            "Iteration 821, loss = 0.03109181\n",
            "Iteration 822, loss = 0.03099163\n",
            "Iteration 823, loss = 0.03091522\n",
            "Iteration 824, loss = 0.03078471\n",
            "Iteration 825, loss = 0.03065439\n",
            "Iteration 826, loss = 0.03052013\n",
            "Iteration 827, loss = 0.03041114\n",
            "Iteration 828, loss = 0.03031268\n",
            "Iteration 829, loss = 0.03023005\n",
            "Iteration 830, loss = 0.03007974\n",
            "Iteration 831, loss = 0.02996478\n",
            "Iteration 832, loss = 0.02988634\n",
            "Iteration 833, loss = 0.02974509\n",
            "Iteration 834, loss = 0.02962845\n",
            "Iteration 835, loss = 0.02953269\n",
            "Iteration 836, loss = 0.02942896\n",
            "Iteration 837, loss = 0.02936684\n",
            "Iteration 838, loss = 0.02925709\n",
            "Iteration 839, loss = 0.02915326\n",
            "Iteration 840, loss = 0.02900932\n",
            "Iteration 841, loss = 0.02891291\n",
            "Iteration 842, loss = 0.02882247\n",
            "Iteration 843, loss = 0.02873128\n",
            "Iteration 844, loss = 0.02864388\n",
            "Iteration 845, loss = 0.02848761\n",
            "Iteration 846, loss = 0.02840417\n",
            "Iteration 847, loss = 0.02835808\n",
            "Iteration 848, loss = 0.02822791\n",
            "Iteration 849, loss = 0.02814413\n",
            "Iteration 850, loss = 0.02801640\n",
            "Iteration 851, loss = 0.02795321\n",
            "Iteration 852, loss = 0.02781506\n",
            "Iteration 853, loss = 0.02773671\n",
            "Iteration 854, loss = 0.02762762\n",
            "Iteration 855, loss = 0.02753762\n",
            "Iteration 856, loss = 0.02744883\n",
            "Iteration 857, loss = 0.02735858\n",
            "Iteration 858, loss = 0.02727434\n",
            "Iteration 859, loss = 0.02720448\n",
            "Iteration 860, loss = 0.02709541\n",
            "Iteration 861, loss = 0.02713018\n",
            "Iteration 862, loss = 0.02690069\n",
            "Iteration 863, loss = 0.02685889\n",
            "Iteration 864, loss = 0.02673855\n",
            "Iteration 865, loss = 0.02665087\n",
            "Iteration 866, loss = 0.02656495\n",
            "Iteration 867, loss = 0.02649178\n",
            "Iteration 868, loss = 0.02637772\n",
            "Iteration 869, loss = 0.02632516\n",
            "Iteration 870, loss = 0.02626071\n",
            "Iteration 871, loss = 0.02618113\n",
            "Iteration 872, loss = 0.02609455\n",
            "Iteration 873, loss = 0.02598470\n",
            "Iteration 874, loss = 0.02586473\n",
            "Iteration 875, loss = 0.02578451\n",
            "Iteration 876, loss = 0.02569439\n",
            "Iteration 877, loss = 0.02565188\n",
            "Iteration 878, loss = 0.02554012\n",
            "Iteration 879, loss = 0.02547562\n",
            "Iteration 880, loss = 0.02544380\n",
            "Iteration 881, loss = 0.02531824\n",
            "Iteration 882, loss = 0.02523994\n",
            "Iteration 883, loss = 0.02516610\n",
            "Iteration 884, loss = 0.02510434\n",
            "Iteration 885, loss = 0.02500813\n",
            "Iteration 886, loss = 0.02493048\n",
            "Iteration 887, loss = 0.02485533\n",
            "Iteration 888, loss = 0.02476743\n",
            "Iteration 889, loss = 0.02470248\n",
            "Iteration 890, loss = 0.02464395\n",
            "Iteration 891, loss = 0.02463089\n",
            "Iteration 892, loss = 0.02450856\n",
            "Iteration 893, loss = 0.02439860\n",
            "Iteration 894, loss = 0.02432960\n",
            "Iteration 895, loss = 0.02426982\n",
            "Iteration 896, loss = 0.02419204\n",
            "Iteration 897, loss = 0.02416801\n",
            "Iteration 898, loss = 0.02405870\n",
            "Iteration 899, loss = 0.02402410\n",
            "Iteration 900, loss = 0.02389259\n",
            "Iteration 901, loss = 0.02383558\n",
            "Iteration 902, loss = 0.02376182\n",
            "Iteration 903, loss = 0.02374511\n",
            "Iteration 904, loss = 0.02362023\n",
            "Iteration 905, loss = 0.02358508\n",
            "Iteration 906, loss = 0.02349490\n",
            "Iteration 907, loss = 0.02341798\n",
            "Iteration 908, loss = 0.02336133\n",
            "Iteration 909, loss = 0.02337568\n",
            "Iteration 910, loss = 0.02322251\n",
            "Iteration 911, loss = 0.02314938\n",
            "Iteration 912, loss = 0.02309020\n",
            "Iteration 913, loss = 0.02302398\n",
            "Iteration 914, loss = 0.02297299\n",
            "Iteration 915, loss = 0.02288428\n",
            "Iteration 916, loss = 0.02282374\n",
            "Iteration 917, loss = 0.02278350\n",
            "Iteration 918, loss = 0.02271489\n",
            "Iteration 919, loss = 0.02268039\n",
            "Iteration 920, loss = 0.02257509\n",
            "Iteration 921, loss = 0.02251267\n",
            "Iteration 922, loss = 0.02247191\n",
            "Iteration 923, loss = 0.02241901\n",
            "Iteration 924, loss = 0.02231684\n",
            "Iteration 925, loss = 0.02227652\n",
            "Iteration 926, loss = 0.02222843\n",
            "Iteration 927, loss = 0.02214685\n",
            "Iteration 928, loss = 0.02211306\n",
            "Iteration 929, loss = 0.02203214\n",
            "Iteration 930, loss = 0.02194806\n",
            "Iteration 931, loss = 0.02192577\n",
            "Iteration 932, loss = 0.02188356\n",
            "Iteration 933, loss = 0.02184257\n",
            "Iteration 934, loss = 0.02176037\n",
            "Iteration 935, loss = 0.02167801\n",
            "Iteration 936, loss = 0.02161820\n",
            "Iteration 937, loss = 0.02160301\n",
            "Iteration 938, loss = 0.02150702\n",
            "Iteration 939, loss = 0.02146315\n",
            "Iteration 940, loss = 0.02140427\n",
            "Iteration 941, loss = 0.02133546\n",
            "Iteration 942, loss = 0.02130763\n",
            "Iteration 943, loss = 0.02127633\n",
            "Iteration 944, loss = 0.02120634\n",
            "Iteration 945, loss = 0.02113604\n",
            "Iteration 946, loss = 0.02110726\n",
            "Iteration 947, loss = 0.02100692\n",
            "Iteration 948, loss = 0.02100206\n",
            "Iteration 949, loss = 0.02090730\n",
            "Iteration 950, loss = 0.02093907\n",
            "Iteration 951, loss = 0.02083045\n",
            "Iteration 952, loss = 0.02076044\n",
            "Iteration 953, loss = 0.02068442\n",
            "Iteration 954, loss = 0.02064026\n",
            "Iteration 955, loss = 0.02060482\n",
            "Iteration 956, loss = 0.02054468\n",
            "Iteration 957, loss = 0.02050250\n",
            "Iteration 958, loss = 0.02045479\n",
            "Iteration 959, loss = 0.02038698\n",
            "Iteration 960, loss = 0.02033358\n",
            "Iteration 961, loss = 0.02027706\n",
            "Iteration 962, loss = 0.02022307\n",
            "Iteration 963, loss = 0.02020938\n",
            "Iteration 964, loss = 0.02014743\n",
            "Iteration 965, loss = 0.02009742\n",
            "Iteration 966, loss = 0.02002071\n",
            "Iteration 967, loss = 0.02001779\n",
            "Iteration 968, loss = 0.01995369\n",
            "Iteration 969, loss = 0.01991427\n",
            "Iteration 970, loss = 0.01984277\n",
            "Iteration 971, loss = 0.01980556\n",
            "Iteration 972, loss = 0.01974091\n",
            "Iteration 973, loss = 0.01971784\n",
            "Iteration 974, loss = 0.01964368\n",
            "Iteration 975, loss = 0.01963696\n",
            "Iteration 976, loss = 0.01959136\n",
            "Iteration 977, loss = 0.01959984\n",
            "Iteration 978, loss = 0.01949894\n",
            "Iteration 979, loss = 0.01944087\n",
            "Iteration 980, loss = 0.01940287\n",
            "Iteration 981, loss = 0.01934632\n",
            "Iteration 982, loss = 0.01929810\n",
            "Iteration 983, loss = 0.01925604\n",
            "Iteration 984, loss = 0.01921779\n",
            "Iteration 985, loss = 0.01920264\n",
            "Iteration 986, loss = 0.01911421\n",
            "Iteration 987, loss = 0.01906486\n",
            "Iteration 988, loss = 0.01904433\n",
            "Iteration 989, loss = 0.01896052\n",
            "Iteration 990, loss = 0.01895786\n",
            "Iteration 991, loss = 0.01891414\n",
            "Iteration 992, loss = 0.01884998\n",
            "Iteration 993, loss = 0.01879982\n",
            "Iteration 994, loss = 0.01875759\n",
            "Iteration 995, loss = 0.01872581\n",
            "Iteration 996, loss = 0.01868388\n",
            "Iteration 997, loss = 0.01875246\n",
            "Iteration 998, loss = 0.01865275\n",
            "Iteration 999, loss = 0.01855836\n",
            "Iteration 1000, loss = 0.01851607\n",
            "Iteration 1001, loss = 0.01845581\n",
            "Iteration 1002, loss = 0.01845932\n",
            "Iteration 1003, loss = 0.01840160\n",
            "Iteration 1004, loss = 0.01838774\n",
            "Iteration 1005, loss = 0.01832392\n",
            "Iteration 1006, loss = 0.01826735\n",
            "Iteration 1007, loss = 0.01821816\n",
            "Iteration 1008, loss = 0.01818304\n",
            "Iteration 1009, loss = 0.01814508\n",
            "Iteration 1010, loss = 0.01811308\n",
            "Iteration 1011, loss = 0.01813683\n",
            "Iteration 1012, loss = 0.01807981\n",
            "Iteration 1013, loss = 0.01800207\n",
            "Iteration 1014, loss = 0.01798357\n",
            "Iteration 1015, loss = 0.01790375\n",
            "Iteration 1016, loss = 0.01787142\n",
            "Iteration 1017, loss = 0.01785419\n",
            "Iteration 1018, loss = 0.01781950\n",
            "Iteration 1019, loss = 0.01776797\n",
            "Iteration 1020, loss = 0.01779486\n",
            "Iteration 1021, loss = 0.01771509\n",
            "Iteration 1022, loss = 0.01766360\n",
            "Iteration 1023, loss = 0.01764349\n",
            "Iteration 1024, loss = 0.01757672\n",
            "Iteration 1025, loss = 0.01753902\n",
            "Iteration 1026, loss = 0.01752664\n",
            "Iteration 1027, loss = 0.01747603\n",
            "Iteration 1028, loss = 0.01743901\n",
            "Iteration 1029, loss = 0.01741707\n",
            "Iteration 1030, loss = 0.01739678\n",
            "Iteration 1031, loss = 0.01739562\n",
            "Iteration 1032, loss = 0.01730987\n",
            "Iteration 1033, loss = 0.01731105\n",
            "Iteration 1034, loss = 0.01724483\n",
            "Iteration 1035, loss = 0.01722670\n",
            "Iteration 1036, loss = 0.01717439\n",
            "Iteration 1037, loss = 0.01712516\n",
            "Iteration 1038, loss = 0.01708043\n",
            "Iteration 1039, loss = 0.01706380\n",
            "Iteration 1040, loss = 0.01697601\n",
            "Iteration 1041, loss = 0.01703067\n",
            "Iteration 1042, loss = 0.01697988\n",
            "Iteration 1043, loss = 0.01691530\n",
            "Iteration 1044, loss = 0.01690027\n",
            "Iteration 1045, loss = 0.01682285\n",
            "Iteration 1046, loss = 0.01681593\n",
            "Iteration 1047, loss = 0.01676921\n",
            "Iteration 1048, loss = 0.01675226\n",
            "Iteration 1049, loss = 0.01672068\n",
            "Iteration 1050, loss = 0.01671775\n",
            "Iteration 1051, loss = 0.01670952\n",
            "Iteration 1052, loss = 0.01663068\n",
            "Iteration 1053, loss = 0.01661274\n",
            "Iteration 1054, loss = 0.01661002\n",
            "Iteration 1055, loss = 0.01652632\n",
            "Iteration 1056, loss = 0.01649868\n",
            "Iteration 1057, loss = 0.01645246\n",
            "Iteration 1058, loss = 0.01644171\n",
            "Iteration 1059, loss = 0.01641740\n",
            "Iteration 1060, loss = 0.01630479\n",
            "Iteration 1061, loss = 0.01634176\n",
            "Iteration 1062, loss = 0.01632367\n",
            "Iteration 1063, loss = 0.01629957\n",
            "Iteration 1064, loss = 0.01624598\n",
            "Iteration 1065, loss = 0.01621907\n",
            "Iteration 1066, loss = 0.01617451\n",
            "Iteration 1067, loss = 0.01614241\n",
            "Iteration 1068, loss = 0.01615487\n",
            "Iteration 1069, loss = 0.01609006\n",
            "Iteration 1070, loss = 0.01604515\n",
            "Iteration 1071, loss = 0.01603345\n",
            "Iteration 1072, loss = 0.01601059\n",
            "Iteration 1073, loss = 0.01596357\n",
            "Iteration 1074, loss = 0.01596078\n",
            "Iteration 1075, loss = 0.01590493\n",
            "Iteration 1076, loss = 0.01593281\n",
            "Iteration 1077, loss = 0.01591164\n",
            "Iteration 1078, loss = 0.01584346\n",
            "Iteration 1079, loss = 0.01578900\n",
            "Iteration 1080, loss = 0.01575878\n",
            "Iteration 1081, loss = 0.01575907\n",
            "Iteration 1082, loss = 0.01569318\n",
            "Iteration 1083, loss = 0.01569748\n",
            "Iteration 1084, loss = 0.01565262\n",
            "Iteration 1085, loss = 0.01562016\n",
            "Iteration 1086, loss = 0.01562044\n",
            "Iteration 1087, loss = 0.01560766\n",
            "Iteration 1088, loss = 0.01552427\n",
            "Iteration 1089, loss = 0.01550646\n",
            "Iteration 1090, loss = 0.01549258\n",
            "Iteration 1091, loss = 0.01549737\n",
            "Iteration 1092, loss = 0.01544983\n",
            "Iteration 1093, loss = 0.01541221\n",
            "Iteration 1094, loss = 0.01536980\n",
            "Iteration 1095, loss = 0.01533000\n",
            "Iteration 1096, loss = 0.01532953\n",
            "Iteration 1097, loss = 0.01530212\n",
            "Iteration 1098, loss = 0.01527975\n",
            "Iteration 1099, loss = 0.01524760\n",
            "Iteration 1100, loss = 0.01521876\n",
            "Iteration 1101, loss = 0.01514886\n",
            "Iteration 1102, loss = 0.01524761\n",
            "Iteration 1103, loss = 0.01511679\n",
            "Iteration 1104, loss = 0.01508604\n",
            "Iteration 1105, loss = 0.01508474\n",
            "Iteration 1106, loss = 0.01506331\n",
            "Iteration 1107, loss = 0.01503199\n",
            "Iteration 1108, loss = 0.01500536\n",
            "Iteration 1109, loss = 0.01503004\n",
            "Iteration 1110, loss = 0.01495300\n",
            "Iteration 1111, loss = 0.01492195\n",
            "Iteration 1112, loss = 0.01492855\n",
            "Iteration 1113, loss = 0.01491788\n",
            "Iteration 1114, loss = 0.01484654\n",
            "Iteration 1115, loss = 0.01488348\n",
            "Iteration 1116, loss = 0.01486080\n",
            "Iteration 1117, loss = 0.01481062\n",
            "Iteration 1118, loss = 0.01473913\n",
            "Iteration 1119, loss = 0.01473856\n",
            "Iteration 1120, loss = 0.01477702\n",
            "Iteration 1121, loss = 0.01466083\n",
            "Iteration 1122, loss = 0.01462945\n",
            "Iteration 1123, loss = 0.01464235\n",
            "Iteration 1124, loss = 0.01458706\n",
            "Iteration 1125, loss = 0.01458816\n",
            "Iteration 1126, loss = 0.01451809\n",
            "Iteration 1127, loss = 0.01454667\n",
            "Iteration 1128, loss = 0.01451410\n",
            "Iteration 1129, loss = 0.01450576\n",
            "Iteration 1130, loss = 0.01444804\n",
            "Iteration 1131, loss = 0.01441592\n",
            "Iteration 1132, loss = 0.01439944\n",
            "Iteration 1133, loss = 0.01436741\n",
            "Iteration 1134, loss = 0.01433325\n",
            "Iteration 1135, loss = 0.01442741\n",
            "Iteration 1136, loss = 0.01433115\n",
            "Iteration 1137, loss = 0.01431734\n",
            "Iteration 1138, loss = 0.01426325\n",
            "Iteration 1139, loss = 0.01425805\n",
            "Iteration 1140, loss = 0.01420624\n",
            "Iteration 1141, loss = 0.01418220\n",
            "Iteration 1142, loss = 0.01417873\n",
            "Iteration 1143, loss = 0.01415325\n",
            "Iteration 1144, loss = 0.01413003\n",
            "Iteration 1145, loss = 0.01407723\n",
            "Iteration 1146, loss = 0.01408150\n",
            "Iteration 1147, loss = 0.01405847\n",
            "Iteration 1148, loss = 0.01403055\n",
            "Iteration 1149, loss = 0.01400783\n",
            "Iteration 1150, loss = 0.01401926\n",
            "Iteration 1151, loss = 0.01397140\n",
            "Iteration 1152, loss = 0.01401764\n",
            "Iteration 1153, loss = 0.01395194\n",
            "Iteration 1154, loss = 0.01387904\n",
            "Iteration 1155, loss = 0.01386548\n",
            "Iteration 1156, loss = 0.01385502\n",
            "Iteration 1157, loss = 0.01384590\n",
            "Iteration 1158, loss = 0.01378717\n",
            "Iteration 1159, loss = 0.01381990\n",
            "Iteration 1160, loss = 0.01377616\n",
            "Iteration 1161, loss = 0.01374784\n",
            "Iteration 1162, loss = 0.01381238\n",
            "Iteration 1163, loss = 0.01370271\n",
            "Iteration 1164, loss = 0.01368821\n",
            "Iteration 1165, loss = 0.01370483\n",
            "Iteration 1166, loss = 0.01365073\n",
            "Iteration 1167, loss = 0.01364840\n",
            "Iteration 1168, loss = 0.01360176\n",
            "Iteration 1169, loss = 0.01359165\n",
            "Iteration 1170, loss = 0.01357313\n",
            "Iteration 1171, loss = 0.01352731\n",
            "Iteration 1172, loss = 0.01359563\n",
            "Iteration 1173, loss = 0.01349844\n",
            "Iteration 1174, loss = 0.01359812\n",
            "Iteration 1175, loss = 0.01351543\n",
            "Iteration 1176, loss = 0.01342862\n",
            "Iteration 1177, loss = 0.01345015\n",
            "Iteration 1178, loss = 0.01340582\n",
            "Iteration 1179, loss = 0.01337176\n",
            "Iteration 1180, loss = 0.01334762\n",
            "Iteration 1181, loss = 0.01332760\n",
            "Iteration 1182, loss = 0.01329530\n",
            "Iteration 1183, loss = 0.01330622\n",
            "Iteration 1184, loss = 0.01333321\n",
            "Iteration 1185, loss = 0.01324501\n",
            "Iteration 1186, loss = 0.01322256\n",
            "Iteration 1187, loss = 0.01323492\n",
            "Iteration 1188, loss = 0.01324479\n",
            "Iteration 1189, loss = 0.01321238\n",
            "Iteration 1190, loss = 0.01315335\n",
            "Iteration 1191, loss = 0.01316944\n",
            "Iteration 1192, loss = 0.01313590\n",
            "Iteration 1193, loss = 0.01309590\n",
            "Iteration 1194, loss = 0.01309001\n",
            "Iteration 1195, loss = 0.01306886\n",
            "Iteration 1196, loss = 0.01306122\n",
            "Iteration 1197, loss = 0.01300306\n",
            "Iteration 1198, loss = 0.01302152\n",
            "Iteration 1199, loss = 0.01301902\n",
            "Iteration 1200, loss = 0.01299318\n",
            "Iteration 1201, loss = 0.01293231\n",
            "Iteration 1202, loss = 0.01293544\n",
            "Iteration 1203, loss = 0.01295132\n",
            "Iteration 1204, loss = 0.01290277\n",
            "Iteration 1205, loss = 0.01290474\n",
            "Iteration 1206, loss = 0.01286766\n",
            "Iteration 1207, loss = 0.01283992\n",
            "Iteration 1208, loss = 0.01282215\n",
            "Iteration 1209, loss = 0.01279560\n",
            "Iteration 1210, loss = 0.01278820\n",
            "Iteration 1211, loss = 0.01276212\n",
            "Iteration 1212, loss = 0.01273277\n",
            "Iteration 1213, loss = 0.01272334\n",
            "Iteration 1214, loss = 0.01270531\n",
            "Iteration 1215, loss = 0.01275450\n",
            "Iteration 1216, loss = 0.01271636\n",
            "Iteration 1217, loss = 0.01266849\n",
            "Iteration 1218, loss = 0.01266680\n",
            "Iteration 1219, loss = 0.01262299\n",
            "Iteration 1220, loss = 0.01260696\n",
            "Iteration 1221, loss = 0.01258821\n",
            "Iteration 1222, loss = 0.01255706\n",
            "Iteration 1223, loss = 0.01253141\n",
            "Iteration 1224, loss = 0.01256440\n",
            "Iteration 1225, loss = 0.01251851\n",
            "Iteration 1226, loss = 0.01250436\n",
            "Iteration 1227, loss = 0.01246027\n",
            "Iteration 1228, loss = 0.01252807\n",
            "Iteration 1229, loss = 0.01240980\n",
            "Iteration 1230, loss = 0.01245563\n",
            "Iteration 1231, loss = 0.01242674\n",
            "Iteration 1232, loss = 0.01242103\n",
            "Iteration 1233, loss = 0.01236874\n",
            "Iteration 1234, loss = 0.01237511\n",
            "Iteration 1235, loss = 0.01233692\n",
            "Iteration 1236, loss = 0.01233907\n",
            "Iteration 1237, loss = 0.01233153\n",
            "Iteration 1238, loss = 0.01228216\n",
            "Iteration 1239, loss = 0.01228406\n",
            "Iteration 1240, loss = 0.01225339\n",
            "Iteration 1241, loss = 0.01223249\n",
            "Iteration 1242, loss = 0.01221822\n",
            "Iteration 1243, loss = 0.01221246\n",
            "Iteration 1244, loss = 0.01226007\n",
            "Iteration 1245, loss = 0.01217683\n",
            "Iteration 1246, loss = 0.01217726\n",
            "Iteration 1247, loss = 0.01219009\n",
            "Iteration 1248, loss = 0.01212854\n",
            "Iteration 1249, loss = 0.01210032\n",
            "Iteration 1250, loss = 0.01207440\n",
            "Iteration 1251, loss = 0.01208221\n",
            "Iteration 1252, loss = 0.01204528\n",
            "Iteration 1253, loss = 0.01213699\n",
            "Iteration 1254, loss = 0.01204483\n",
            "Iteration 1255, loss = 0.01202392\n",
            "Iteration 1256, loss = 0.01205132\n",
            "Iteration 1257, loss = 0.01197419\n",
            "Iteration 1258, loss = 0.01198314\n",
            "Iteration 1259, loss = 0.01202139\n",
            "Iteration 1260, loss = 0.01195378\n",
            "Iteration 1261, loss = 0.01192661\n",
            "Iteration 1262, loss = 0.01189457\n",
            "Iteration 1263, loss = 0.01190248\n",
            "Iteration 1264, loss = 0.01188512\n",
            "Iteration 1265, loss = 0.01185452\n",
            "Iteration 1266, loss = 0.01185825\n",
            "Iteration 1267, loss = 0.01185608\n",
            "Iteration 1268, loss = 0.01181365\n",
            "Iteration 1269, loss = 0.01176769\n",
            "Iteration 1270, loss = 0.01175203\n",
            "Iteration 1271, loss = 0.01177343\n",
            "Iteration 1272, loss = 0.01174974\n",
            "Iteration 1273, loss = 0.01174623\n",
            "Iteration 1274, loss = 0.01169491\n",
            "Iteration 1275, loss = 0.01171606\n",
            "Iteration 1276, loss = 0.01171242\n",
            "Iteration 1277, loss = 0.01167038\n",
            "Iteration 1278, loss = 0.01161132\n",
            "Iteration 1279, loss = 0.01164131\n",
            "Iteration 1280, loss = 0.01167623\n",
            "Iteration 1281, loss = 0.01164112\n",
            "Iteration 1282, loss = 0.01159086\n",
            "Iteration 1283, loss = 0.01154461\n",
            "Iteration 1284, loss = 0.01159658\n",
            "Iteration 1285, loss = 0.01154073\n",
            "Iteration 1286, loss = 0.01155730\n",
            "Iteration 1287, loss = 0.01150364\n",
            "Iteration 1288, loss = 0.01152196\n",
            "Iteration 1289, loss = 0.01147606\n",
            "Iteration 1290, loss = 0.01145661\n",
            "Iteration 1291, loss = 0.01143793\n",
            "Iteration 1292, loss = 0.01145530\n",
            "Iteration 1293, loss = 0.01147844\n",
            "Iteration 1294, loss = 0.01141852\n",
            "Iteration 1295, loss = 0.01141848\n",
            "Iteration 1296, loss = 0.01138506\n",
            "Iteration 1297, loss = 0.01139468\n",
            "Iteration 1298, loss = 0.01133441\n",
            "Iteration 1299, loss = 0.01139490\n",
            "Iteration 1300, loss = 0.01133279\n",
            "Iteration 1301, loss = 0.01134090\n",
            "Iteration 1302, loss = 0.01128185\n",
            "Iteration 1303, loss = 0.01127825\n",
            "Iteration 1304, loss = 0.01127079\n",
            "Iteration 1305, loss = 0.01127216\n",
            "Iteration 1306, loss = 0.01123608\n",
            "Iteration 1307, loss = 0.01121543\n",
            "Iteration 1308, loss = 0.01120708\n",
            "Iteration 1309, loss = 0.01117345\n",
            "Iteration 1310, loss = 0.01117973\n",
            "Iteration 1311, loss = 0.01114611\n",
            "Iteration 1312, loss = 0.01116347\n",
            "Iteration 1313, loss = 0.01115130\n",
            "Iteration 1314, loss = 0.01113368\n",
            "Iteration 1315, loss = 0.01112962\n",
            "Iteration 1316, loss = 0.01111029\n",
            "Iteration 1317, loss = 0.01105658\n",
            "Iteration 1318, loss = 0.01110160\n",
            "Iteration 1319, loss = 0.01103987\n",
            "Iteration 1320, loss = 0.01102919\n",
            "Iteration 1321, loss = 0.01101138\n",
            "Iteration 1322, loss = 0.01099936\n",
            "Iteration 1323, loss = 0.01099616\n",
            "Iteration 1324, loss = 0.01096978\n",
            "Iteration 1325, loss = 0.01104906\n",
            "Iteration 1326, loss = 0.01098056\n",
            "Iteration 1327, loss = 0.01091656\n",
            "Iteration 1328, loss = 0.01092994\n",
            "Iteration 1329, loss = 0.01093701\n",
            "Iteration 1330, loss = 0.01089267\n",
            "Iteration 1331, loss = 0.01099939\n",
            "Iteration 1332, loss = 0.01089081\n",
            "Iteration 1333, loss = 0.01086010\n",
            "Iteration 1334, loss = 0.01079983\n",
            "Iteration 1335, loss = 0.01088619\n",
            "Iteration 1336, loss = 0.01087699\n",
            "Iteration 1337, loss = 0.01083851\n",
            "Iteration 1338, loss = 0.01080704\n",
            "Iteration 1339, loss = 0.01077421\n",
            "Iteration 1340, loss = 0.01075407\n",
            "Iteration 1341, loss = 0.01079846\n",
            "Iteration 1342, loss = 0.01074471\n",
            "Iteration 1343, loss = 0.01072431\n",
            "Iteration 1344, loss = 0.01068675\n",
            "Iteration 1345, loss = 0.01067650\n",
            "Iteration 1346, loss = 0.01068733\n",
            "Iteration 1347, loss = 0.01068145\n",
            "Iteration 1348, loss = 0.01064124\n",
            "Iteration 1349, loss = 0.01063214\n",
            "Iteration 1350, loss = 0.01063534\n",
            "Iteration 1351, loss = 0.01064775\n",
            "Iteration 1352, loss = 0.01059593\n",
            "Iteration 1353, loss = 0.01064726\n",
            "Iteration 1354, loss = 0.01057909\n",
            "Iteration 1355, loss = 0.01053362\n",
            "Iteration 1356, loss = 0.01057354\n",
            "Iteration 1357, loss = 0.01054434\n",
            "Iteration 1358, loss = 0.01058425\n",
            "Iteration 1359, loss = 0.01052162\n",
            "Iteration 1360, loss = 0.01048141\n",
            "Iteration 1361, loss = 0.01049034\n",
            "Iteration 1362, loss = 0.01048403\n",
            "Iteration 1363, loss = 0.01053781\n",
            "Iteration 1364, loss = 0.01044379\n",
            "Iteration 1365, loss = 0.01045764\n",
            "Iteration 1366, loss = 0.01041455\n",
            "Iteration 1367, loss = 0.01045312\n",
            "Iteration 1368, loss = 0.01038357\n",
            "Iteration 1369, loss = 0.01037695\n",
            "Iteration 1370, loss = 0.01039454\n",
            "Iteration 1371, loss = 0.01041600\n",
            "Iteration 1372, loss = 0.01036228\n",
            "Iteration 1373, loss = 0.01033794\n",
            "Iteration 1374, loss = 0.01035648\n",
            "Iteration 1375, loss = 0.01034120\n",
            "Iteration 1376, loss = 0.01035038\n",
            "Iteration 1377, loss = 0.01033833\n",
            "Iteration 1378, loss = 0.01027838\n",
            "Iteration 1379, loss = 0.01026701\n",
            "Iteration 1380, loss = 0.01024340\n",
            "Iteration 1381, loss = 0.01026117\n",
            "Iteration 1382, loss = 0.01030969\n",
            "Iteration 1383, loss = 0.01026196\n",
            "Iteration 1384, loss = 0.01023702\n",
            "Iteration 1385, loss = 0.01021655\n",
            "Iteration 1386, loss = 0.01018610\n",
            "Iteration 1387, loss = 0.01019343\n",
            "Iteration 1388, loss = 0.01021644\n",
            "Iteration 1389, loss = 0.01025074\n",
            "Iteration 1390, loss = 0.01016820\n",
            "Iteration 1391, loss = 0.01014973\n",
            "Iteration 1392, loss = 0.01015511\n",
            "Iteration 1393, loss = 0.01009575\n",
            "Iteration 1394, loss = 0.01008111\n",
            "Iteration 1395, loss = 0.01005696\n",
            "Iteration 1396, loss = 0.01005055\n",
            "Iteration 1397, loss = 0.01004517\n",
            "Iteration 1398, loss = 0.01008280\n",
            "Iteration 1399, loss = 0.01004247\n",
            "Iteration 1400, loss = 0.01004641\n",
            "Iteration 1401, loss = 0.01002724\n",
            "Iteration 1402, loss = 0.01005505\n",
            "Iteration 1403, loss = 0.01001754\n",
            "Iteration 1404, loss = 0.00997441\n",
            "Iteration 1405, loss = 0.01002084\n",
            "Iteration 1406, loss = 0.00991644\n",
            "Iteration 1407, loss = 0.00993036\n",
            "Iteration 1408, loss = 0.00997302\n",
            "Iteration 1409, loss = 0.00990382\n",
            "Iteration 1410, loss = 0.00991137\n",
            "Iteration 1411, loss = 0.00990633\n",
            "Iteration 1412, loss = 0.00988113\n",
            "Iteration 1413, loss = 0.00986550\n",
            "Iteration 1414, loss = 0.00987504\n",
            "Iteration 1415, loss = 0.00988092\n",
            "Iteration 1416, loss = 0.00982783\n",
            "Iteration 1417, loss = 0.00982723\n",
            "Iteration 1418, loss = 0.00979050\n",
            "Iteration 1419, loss = 0.00983786\n",
            "Iteration 1420, loss = 0.00982846\n",
            "Iteration 1421, loss = 0.00981711\n",
            "Iteration 1422, loss = 0.00978272\n",
            "Iteration 1423, loss = 0.00980668\n",
            "Iteration 1424, loss = 0.00972349\n",
            "Iteration 1425, loss = 0.00975685\n",
            "Iteration 1426, loss = 0.00975257\n",
            "Iteration 1427, loss = 0.00975773\n",
            "Iteration 1428, loss = 0.00973538\n",
            "Iteration 1429, loss = 0.00969773\n",
            "Iteration 1430, loss = 0.00974081\n",
            "Iteration 1431, loss = 0.00970375\n",
            "Iteration 1432, loss = 0.00969022\n",
            "Iteration 1433, loss = 0.00968908\n",
            "Iteration 1434, loss = 0.00971629\n",
            "Iteration 1435, loss = 0.00968577\n",
            "Iteration 1436, loss = 0.00969796\n",
            "Iteration 1437, loss = 0.00966067\n",
            "Iteration 1438, loss = 0.00965212\n",
            "Iteration 1439, loss = 0.00963274\n",
            "Iteration 1440, loss = 0.00960424\n",
            "Iteration 1441, loss = 0.00957143\n",
            "Iteration 1442, loss = 0.00957106\n",
            "Iteration 1443, loss = 0.00954651\n",
            "Iteration 1444, loss = 0.00953241\n",
            "Iteration 1445, loss = 0.00952557\n",
            "Iteration 1446, loss = 0.00957147\n",
            "Iteration 1447, loss = 0.00950895\n",
            "Iteration 1448, loss = 0.00952132\n",
            "Iteration 1449, loss = 0.00948468\n",
            "Iteration 1450, loss = 0.00948322\n",
            "Iteration 1451, loss = 0.00949621\n",
            "Iteration 1452, loss = 0.00947521\n",
            "Iteration 1453, loss = 0.00945249\n",
            "Iteration 1454, loss = 0.00944043\n",
            "Iteration 1455, loss = 0.00946055\n",
            "Iteration 1456, loss = 0.00942631\n",
            "Iteration 1457, loss = 0.00943030\n",
            "Iteration 1458, loss = 0.00940812\n",
            "Iteration 1459, loss = 0.00938578\n",
            "Iteration 1460, loss = 0.00946348\n",
            "Iteration 1461, loss = 0.00941163\n",
            "Iteration 1462, loss = 0.00938368\n",
            "Iteration 1463, loss = 0.00934163\n",
            "Iteration 1464, loss = 0.00947768\n",
            "Iteration 1465, loss = 0.00935311\n",
            "Iteration 1466, loss = 0.00931979\n",
            "Iteration 1467, loss = 0.00932199\n",
            "Iteration 1468, loss = 0.00930941\n",
            "Iteration 1469, loss = 0.00927276\n",
            "Iteration 1470, loss = 0.00928933\n",
            "Iteration 1471, loss = 0.00926872\n",
            "Iteration 1472, loss = 0.00927689\n",
            "Iteration 1473, loss = 0.00925843\n",
            "Iteration 1474, loss = 0.00923368\n",
            "Iteration 1475, loss = 0.00924344\n",
            "Iteration 1476, loss = 0.00925992\n",
            "Iteration 1477, loss = 0.00925479\n",
            "Iteration 1478, loss = 0.00926831\n",
            "Iteration 1479, loss = 0.00923358\n",
            "Iteration 1480, loss = 0.00917609\n",
            "Iteration 1481, loss = 0.00922890\n",
            "Iteration 1482, loss = 0.00921490\n",
            "Iteration 1483, loss = 0.00916203\n",
            "Iteration 1484, loss = 0.00915460\n",
            "Iteration 1485, loss = 0.00913824\n",
            "Iteration 1486, loss = 0.00920338\n",
            "Iteration 1487, loss = 0.00911050\n",
            "Iteration 1488, loss = 0.00910324\n",
            "Iteration 1489, loss = 0.00911330\n",
            "Iteration 1490, loss = 0.00912309\n",
            "Iteration 1491, loss = 0.00917686\n",
            "Iteration 1492, loss = 0.00908642\n",
            "Iteration 1493, loss = 0.00907589\n",
            "Iteration 1494, loss = 0.00905230\n",
            "Iteration 1495, loss = 0.00904443\n",
            "Iteration 1496, loss = 0.00907149\n",
            "Iteration 1497, loss = 0.00906703\n",
            "Iteration 1498, loss = 0.00904392\n",
            "Iteration 1499, loss = 0.00904171\n",
            "Iteration 1500, loss = 0.00902003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0ojhOTsXIQk",
        "outputId": "ef3e2415-b171-40b4-a46c-b505452b15e4"
      },
      "source": [
        "print(classification_report(y_credit_teste, previsoes))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       0.98      1.00      0.99        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       0.99      1.00      1.00       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAhVZUHDXULt"
      },
      "source": [
        "###Base Census"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqWyVrqdXS-u"
      },
      "source": [
        "with open('census.pkl', 'rb') as f:\n",
        "  X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD_zsnyRXpBj",
        "outputId": "7b0c2808-586f-4cc0-b0d6-380cdfb225db"
      },
      "source": [
        "X_census_treinamento.shape, y_census_treinamento.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((27676, 108), (27676,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjs8oAfrXuaF",
        "outputId": "7d983b1e-4227-4877-9bc6-fbcead861896"
      },
      "source": [
        "X_census_teste.shape, y_census_teste.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4885, 108), (4885,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n80tYe1iYUps",
        "outputId": "8018a542-e702-4509-a0c5-2f90869030aa"
      },
      "source": [
        "rede_neural_census = MLPClassifier(verbose=True, max_iter=1000, tol=0.000010,\n",
        "                                   hidden_layer_sizes = (55, 55))\n",
        "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.39675627\n",
            "Iteration 2, loss = 0.32851858\n",
            "Iteration 3, loss = 0.31630177\n",
            "Iteration 4, loss = 0.30830551\n",
            "Iteration 5, loss = 0.30317673\n",
            "Iteration 6, loss = 0.29916639\n",
            "Iteration 7, loss = 0.29589433\n",
            "Iteration 8, loss = 0.29385584\n",
            "Iteration 9, loss = 0.29082221\n",
            "Iteration 10, loss = 0.28794345\n",
            "Iteration 11, loss = 0.28609695\n",
            "Iteration 12, loss = 0.28469786\n",
            "Iteration 13, loss = 0.28215754\n",
            "Iteration 14, loss = 0.27974220\n",
            "Iteration 15, loss = 0.27876472\n",
            "Iteration 16, loss = 0.27648578\n",
            "Iteration 17, loss = 0.27541329\n",
            "Iteration 18, loss = 0.27409357\n",
            "Iteration 19, loss = 0.27183680\n",
            "Iteration 20, loss = 0.26991863\n",
            "Iteration 21, loss = 0.26810368\n",
            "Iteration 22, loss = 0.26722979\n",
            "Iteration 23, loss = 0.26614638\n",
            "Iteration 24, loss = 0.26382382\n",
            "Iteration 25, loss = 0.26288727\n",
            "Iteration 26, loss = 0.26014977\n",
            "Iteration 27, loss = 0.26014919\n",
            "Iteration 28, loss = 0.25875803\n",
            "Iteration 29, loss = 0.25695743\n",
            "Iteration 30, loss = 0.25501109\n",
            "Iteration 31, loss = 0.25401635\n",
            "Iteration 32, loss = 0.25244059\n",
            "Iteration 33, loss = 0.25120570\n",
            "Iteration 34, loss = 0.25096926\n",
            "Iteration 35, loss = 0.24880132\n",
            "Iteration 36, loss = 0.24836380\n",
            "Iteration 37, loss = 0.24802059\n",
            "Iteration 38, loss = 0.24530685\n",
            "Iteration 39, loss = 0.24435993\n",
            "Iteration 40, loss = 0.24380431\n",
            "Iteration 41, loss = 0.24184086\n",
            "Iteration 42, loss = 0.24148107\n",
            "Iteration 43, loss = 0.23962556\n",
            "Iteration 44, loss = 0.23933158\n",
            "Iteration 45, loss = 0.23878837\n",
            "Iteration 46, loss = 0.23687230\n",
            "Iteration 47, loss = 0.23664553\n",
            "Iteration 48, loss = 0.23555555\n",
            "Iteration 49, loss = 0.23344711\n",
            "Iteration 50, loss = 0.23339637\n",
            "Iteration 51, loss = 0.23235802\n",
            "Iteration 52, loss = 0.23086201\n",
            "Iteration 53, loss = 0.22974310\n",
            "Iteration 54, loss = 0.22929009\n",
            "Iteration 55, loss = 0.22793070\n",
            "Iteration 56, loss = 0.22727322\n",
            "Iteration 57, loss = 0.22641382\n",
            "Iteration 58, loss = 0.22427329\n",
            "Iteration 59, loss = 0.22456673\n",
            "Iteration 60, loss = 0.22414448\n",
            "Iteration 61, loss = 0.22291483\n",
            "Iteration 62, loss = 0.22136170\n",
            "Iteration 63, loss = 0.22192043\n",
            "Iteration 64, loss = 0.21994461\n",
            "Iteration 65, loss = 0.21944132\n",
            "Iteration 66, loss = 0.21797893\n",
            "Iteration 67, loss = 0.21783875\n",
            "Iteration 68, loss = 0.21672759\n",
            "Iteration 69, loss = 0.21728381\n",
            "Iteration 70, loss = 0.21586692\n",
            "Iteration 71, loss = 0.21586307\n",
            "Iteration 72, loss = 0.21500150\n",
            "Iteration 73, loss = 0.21382276\n",
            "Iteration 74, loss = 0.21507126\n",
            "Iteration 75, loss = 0.21202523\n",
            "Iteration 76, loss = 0.21199355\n",
            "Iteration 77, loss = 0.21076877\n",
            "Iteration 78, loss = 0.21077109\n",
            "Iteration 79, loss = 0.21075542\n",
            "Iteration 80, loss = 0.21011153\n",
            "Iteration 81, loss = 0.20865788\n",
            "Iteration 82, loss = 0.20985546\n",
            "Iteration 83, loss = 0.20686730\n",
            "Iteration 84, loss = 0.20703530\n",
            "Iteration 85, loss = 0.20570701\n",
            "Iteration 86, loss = 0.20609099\n",
            "Iteration 87, loss = 0.20440373\n",
            "Iteration 88, loss = 0.20468825\n",
            "Iteration 89, loss = 0.20425331\n",
            "Iteration 90, loss = 0.20261931\n",
            "Iteration 91, loss = 0.20268379\n",
            "Iteration 92, loss = 0.20273753\n",
            "Iteration 93, loss = 0.20173734\n",
            "Iteration 94, loss = 0.20099557\n",
            "Iteration 95, loss = 0.20010444\n",
            "Iteration 96, loss = 0.20032876\n",
            "Iteration 97, loss = 0.20062421\n",
            "Iteration 98, loss = 0.19954620\n",
            "Iteration 99, loss = 0.19902932\n",
            "Iteration 100, loss = 0.19798240\n",
            "Iteration 101, loss = 0.19785203\n",
            "Iteration 102, loss = 0.19812436\n",
            "Iteration 103, loss = 0.19875597\n",
            "Iteration 104, loss = 0.19587770\n",
            "Iteration 105, loss = 0.19617018\n",
            "Iteration 106, loss = 0.19579781\n",
            "Iteration 107, loss = 0.19551274\n",
            "Iteration 108, loss = 0.19429548\n",
            "Iteration 109, loss = 0.19356052\n",
            "Iteration 110, loss = 0.19441854\n",
            "Iteration 111, loss = 0.19430132\n",
            "Iteration 112, loss = 0.19420031\n",
            "Iteration 113, loss = 0.19115876\n",
            "Iteration 114, loss = 0.19235716\n",
            "Iteration 115, loss = 0.19287373\n",
            "Iteration 116, loss = 0.19197061\n",
            "Iteration 117, loss = 0.19149225\n",
            "Iteration 118, loss = 0.19157420\n",
            "Iteration 119, loss = 0.19046639\n",
            "Iteration 120, loss = 0.18971214\n",
            "Iteration 121, loss = 0.18925729\n",
            "Iteration 122, loss = 0.18831923\n",
            "Iteration 123, loss = 0.18844799\n",
            "Iteration 124, loss = 0.18928258\n",
            "Iteration 125, loss = 0.18757037\n",
            "Iteration 126, loss = 0.18699180\n",
            "Iteration 127, loss = 0.18763269\n",
            "Iteration 128, loss = 0.18690311\n",
            "Iteration 129, loss = 0.18551746\n",
            "Iteration 130, loss = 0.18588938\n",
            "Iteration 131, loss = 0.18632081\n",
            "Iteration 132, loss = 0.18631376\n",
            "Iteration 133, loss = 0.18472339\n",
            "Iteration 134, loss = 0.18417152\n",
            "Iteration 135, loss = 0.18579491\n",
            "Iteration 136, loss = 0.18529178\n",
            "Iteration 137, loss = 0.18416496\n",
            "Iteration 138, loss = 0.18377404\n",
            "Iteration 139, loss = 0.18330004\n",
            "Iteration 140, loss = 0.18201901\n",
            "Iteration 141, loss = 0.18170585\n",
            "Iteration 142, loss = 0.18372362\n",
            "Iteration 143, loss = 0.18389900\n",
            "Iteration 144, loss = 0.18150710\n",
            "Iteration 145, loss = 0.18152793\n",
            "Iteration 146, loss = 0.18058845\n",
            "Iteration 147, loss = 0.18203353\n",
            "Iteration 148, loss = 0.18221467\n",
            "Iteration 149, loss = 0.18195308\n",
            "Iteration 150, loss = 0.17948373\n",
            "Iteration 151, loss = 0.18255279\n",
            "Iteration 152, loss = 0.17891925\n",
            "Iteration 153, loss = 0.17868819\n",
            "Iteration 154, loss = 0.17748578\n",
            "Iteration 155, loss = 0.17749985\n",
            "Iteration 156, loss = 0.17692628\n",
            "Iteration 157, loss = 0.17664530\n",
            "Iteration 158, loss = 0.17701460\n",
            "Iteration 159, loss = 0.17860115\n",
            "Iteration 160, loss = 0.17704306\n",
            "Iteration 161, loss = 0.17544296\n",
            "Iteration 162, loss = 0.17869369\n",
            "Iteration 163, loss = 0.17710459\n",
            "Iteration 164, loss = 0.17654363\n",
            "Iteration 165, loss = 0.17436110\n",
            "Iteration 166, loss = 0.17448503\n",
            "Iteration 167, loss = 0.17510506\n",
            "Iteration 168, loss = 0.17414145\n",
            "Iteration 169, loss = 0.17418737\n",
            "Iteration 170, loss = 0.17350365\n",
            "Iteration 171, loss = 0.17419730\n",
            "Iteration 172, loss = 0.17303283\n",
            "Iteration 173, loss = 0.17335382\n",
            "Iteration 174, loss = 0.17362741\n",
            "Iteration 175, loss = 0.17252111\n",
            "Iteration 176, loss = 0.17362242\n",
            "Iteration 177, loss = 0.17171862\n",
            "Iteration 178, loss = 0.17347436\n",
            "Iteration 179, loss = 0.17381908\n",
            "Iteration 180, loss = 0.17163572\n",
            "Iteration 181, loss = 0.17155524\n",
            "Iteration 182, loss = 0.17294736\n",
            "Iteration 183, loss = 0.17085222\n",
            "Iteration 184, loss = 0.17189619\n",
            "Iteration 185, loss = 0.17083343\n",
            "Iteration 186, loss = 0.17063717\n",
            "Iteration 187, loss = 0.17081259\n",
            "Iteration 188, loss = 0.17029771\n",
            "Iteration 189, loss = 0.17156567\n",
            "Iteration 190, loss = 0.16963844\n",
            "Iteration 191, loss = 0.16835899\n",
            "Iteration 192, loss = 0.16872048\n",
            "Iteration 193, loss = 0.16909514\n",
            "Iteration 194, loss = 0.16900241\n",
            "Iteration 195, loss = 0.16854780\n",
            "Iteration 196, loss = 0.16791456\n",
            "Iteration 197, loss = 0.16850899\n",
            "Iteration 198, loss = 0.16645577\n",
            "Iteration 199, loss = 0.16782312\n",
            "Iteration 200, loss = 0.16857790\n",
            "Iteration 201, loss = 0.16698748\n",
            "Iteration 202, loss = 0.16737766\n",
            "Iteration 203, loss = 0.16726450\n",
            "Iteration 204, loss = 0.16658606\n",
            "Iteration 205, loss = 0.16519242\n",
            "Iteration 206, loss = 0.16496467\n",
            "Iteration 207, loss = 0.16610817\n",
            "Iteration 208, loss = 0.16493942\n",
            "Iteration 209, loss = 0.16507680\n",
            "Iteration 210, loss = 0.16780405\n",
            "Iteration 211, loss = 0.16639005\n",
            "Iteration 212, loss = 0.16428580\n",
            "Iteration 213, loss = 0.16378516\n",
            "Iteration 214, loss = 0.16336814\n",
            "Iteration 215, loss = 0.16359650\n",
            "Iteration 216, loss = 0.16586375\n",
            "Iteration 217, loss = 0.16246917\n",
            "Iteration 218, loss = 0.16175214\n",
            "Iteration 219, loss = 0.16290269\n",
            "Iteration 220, loss = 0.16401393\n",
            "Iteration 221, loss = 0.16263336\n",
            "Iteration 222, loss = 0.16296948\n",
            "Iteration 223, loss = 0.16475664\n",
            "Iteration 224, loss = 0.16211671\n",
            "Iteration 225, loss = 0.16282306\n",
            "Iteration 226, loss = 0.16293568\n",
            "Iteration 227, loss = 0.16033143\n",
            "Iteration 228, loss = 0.16248713\n",
            "Iteration 229, loss = 0.16001836\n",
            "Iteration 230, loss = 0.15964668\n",
            "Iteration 231, loss = 0.16047667\n",
            "Iteration 232, loss = 0.16065803\n",
            "Iteration 233, loss = 0.15884470\n",
            "Iteration 234, loss = 0.16194763\n",
            "Iteration 235, loss = 0.15924445\n",
            "Iteration 236, loss = 0.16055744\n",
            "Iteration 237, loss = 0.15853047\n",
            "Iteration 238, loss = 0.15882823\n",
            "Iteration 239, loss = 0.15838349\n",
            "Iteration 240, loss = 0.15838515\n",
            "Iteration 241, loss = 0.15844788\n",
            "Iteration 242, loss = 0.15982944\n",
            "Iteration 243, loss = 0.15931758\n",
            "Iteration 244, loss = 0.15720591\n",
            "Iteration 245, loss = 0.15820075\n",
            "Iteration 246, loss = 0.15863829\n",
            "Iteration 247, loss = 0.15776337\n",
            "Iteration 248, loss = 0.15830664\n",
            "Iteration 249, loss = 0.15780624\n",
            "Iteration 250, loss = 0.15681594\n",
            "Iteration 251, loss = 0.15824678\n",
            "Iteration 252, loss = 0.15688610\n",
            "Iteration 253, loss = 0.15801993\n",
            "Iteration 254, loss = 0.15515837\n",
            "Iteration 255, loss = 0.15673152\n",
            "Iteration 256, loss = 0.15822792\n",
            "Iteration 257, loss = 0.15720203\n",
            "Iteration 258, loss = 0.15632763\n",
            "Iteration 259, loss = 0.15518978\n",
            "Iteration 260, loss = 0.15688987\n",
            "Iteration 261, loss = 0.15490597\n",
            "Iteration 262, loss = 0.15569577\n",
            "Iteration 263, loss = 0.15495951\n",
            "Iteration 264, loss = 0.15436991\n",
            "Iteration 265, loss = 0.15560364\n",
            "Iteration 266, loss = 0.15555997\n",
            "Iteration 267, loss = 0.15370757\n",
            "Iteration 268, loss = 0.15434192\n",
            "Iteration 269, loss = 0.15597111\n",
            "Iteration 270, loss = 0.15475821\n",
            "Iteration 271, loss = 0.15428383\n",
            "Iteration 272, loss = 0.15270056\n",
            "Iteration 273, loss = 0.15381772\n",
            "Iteration 274, loss = 0.15348845\n",
            "Iteration 275, loss = 0.15400840\n",
            "Iteration 276, loss = 0.15362546\n",
            "Iteration 277, loss = 0.15278985\n",
            "Iteration 278, loss = 0.15243951\n",
            "Iteration 279, loss = 0.15231124\n",
            "Iteration 280, loss = 0.15358482\n",
            "Iteration 281, loss = 0.15245837\n",
            "Iteration 282, loss = 0.15401995\n",
            "Iteration 283, loss = 0.15371500\n",
            "Iteration 284, loss = 0.15515342\n",
            "Iteration 285, loss = 0.15251753\n",
            "Iteration 286, loss = 0.15156083\n",
            "Iteration 287, loss = 0.15061450\n",
            "Iteration 288, loss = 0.15132422\n",
            "Iteration 289, loss = 0.15174695\n",
            "Iteration 290, loss = 0.15029718\n",
            "Iteration 291, loss = 0.15141277\n",
            "Iteration 292, loss = 0.15068019\n",
            "Iteration 293, loss = 0.14989185\n",
            "Iteration 294, loss = 0.14927431\n",
            "Iteration 295, loss = 0.15138870\n",
            "Iteration 296, loss = 0.15216987\n",
            "Iteration 297, loss = 0.14971015\n",
            "Iteration 298, loss = 0.15244562\n",
            "Iteration 299, loss = 0.14979195\n",
            "Iteration 300, loss = 0.15078682\n",
            "Iteration 301, loss = 0.15017212\n",
            "Iteration 302, loss = 0.14966203\n",
            "Iteration 303, loss = 0.14918733\n",
            "Iteration 304, loss = 0.15054906\n",
            "Iteration 305, loss = 0.15033877\n",
            "Iteration 306, loss = 0.14914241\n",
            "Iteration 307, loss = 0.14890386\n",
            "Iteration 308, loss = 0.14786052\n",
            "Iteration 309, loss = 0.14842426\n",
            "Iteration 310, loss = 0.14865154\n",
            "Iteration 311, loss = 0.14829324\n",
            "Iteration 312, loss = 0.14816186\n",
            "Iteration 313, loss = 0.14797819\n",
            "Iteration 314, loss = 0.14942130\n",
            "Iteration 315, loss = 0.15008556\n",
            "Iteration 316, loss = 0.14821706\n",
            "Iteration 317, loss = 0.14778135\n",
            "Iteration 318, loss = 0.14649140\n",
            "Iteration 319, loss = 0.14709949\n",
            "Iteration 320, loss = 0.14737405\n",
            "Iteration 321, loss = 0.14628245\n",
            "Iteration 322, loss = 0.14654939\n",
            "Iteration 323, loss = 0.14794817\n",
            "Iteration 324, loss = 0.14609939\n",
            "Iteration 325, loss = 0.14543887\n",
            "Iteration 326, loss = 0.14665149\n",
            "Iteration 327, loss = 0.14555900\n",
            "Iteration 328, loss = 0.14496157\n",
            "Iteration 329, loss = 0.14596554\n",
            "Iteration 330, loss = 0.14626258\n",
            "Iteration 331, loss = 0.14757968\n",
            "Iteration 332, loss = 0.14628987\n",
            "Iteration 333, loss = 0.14687816\n",
            "Iteration 334, loss = 0.14776770\n",
            "Iteration 335, loss = 0.14483550\n",
            "Iteration 336, loss = 0.14542819\n",
            "Iteration 337, loss = 0.14270827\n",
            "Iteration 338, loss = 0.14425799\n",
            "Iteration 339, loss = 0.14495712\n",
            "Iteration 340, loss = 0.14412646\n",
            "Iteration 341, loss = 0.14538190\n",
            "Iteration 342, loss = 0.14609136\n",
            "Iteration 343, loss = 0.14526533\n",
            "Iteration 344, loss = 0.14396707\n",
            "Iteration 345, loss = 0.14327839\n",
            "Iteration 346, loss = 0.14340667\n",
            "Iteration 347, loss = 0.14494377\n",
            "Iteration 348, loss = 0.14405405\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(55, 55), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miGMEh0OZhBD",
        "outputId": "7e6ff7ef-badd-41bd-d95e-5c493d84b074"
      },
      "source": [
        "previsoes = rede_neural_census.predict(X_census_teste)\n",
        "previsoes"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
              "      dtype='<U6')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whwzRz3eZpR0",
        "outputId": "6391c68e-54da-4fc1-d37f-e675442323fd"
      },
      "source": [
        "y_census_teste"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhWTjRioZuOh",
        "outputId": "a5fda39c-4ade-4542-b456-2b95ae55e984"
      },
      "source": [
        "accuracy_score(y_census_teste, previsoes)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8114636642784033"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1us6gIgcaANd",
        "outputId": "65407c89-d6c6-4761-ca0a-8efa0d28aa9b"
      },
      "source": [
        "cm = ConfusionMatrix(rede_neural_census)\n",
        "cm.fit(X_census_treinamento, y_census_treinamento)\n",
        "cm.score(X_census_teste, y_census_teste)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.38570698\n",
            "Iteration 2, loss = 0.32485782\n",
            "Iteration 3, loss = 0.31411094\n",
            "Iteration 4, loss = 0.30778857\n",
            "Iteration 5, loss = 0.30157063\n",
            "Iteration 6, loss = 0.29841304\n",
            "Iteration 7, loss = 0.29477861\n",
            "Iteration 8, loss = 0.29207883\n",
            "Iteration 9, loss = 0.29015656\n",
            "Iteration 10, loss = 0.28819224\n",
            "Iteration 11, loss = 0.28586912\n",
            "Iteration 12, loss = 0.28425904\n",
            "Iteration 13, loss = 0.28275599\n",
            "Iteration 14, loss = 0.28067698\n",
            "Iteration 15, loss = 0.27844476\n",
            "Iteration 16, loss = 0.27724665\n",
            "Iteration 17, loss = 0.27594140\n",
            "Iteration 18, loss = 0.27447208\n",
            "Iteration 19, loss = 0.27360625\n",
            "Iteration 20, loss = 0.27151495\n",
            "Iteration 21, loss = 0.26983687\n",
            "Iteration 22, loss = 0.26901145\n",
            "Iteration 23, loss = 0.26677707\n",
            "Iteration 24, loss = 0.26597890\n",
            "Iteration 25, loss = 0.26425785\n",
            "Iteration 26, loss = 0.26276224\n",
            "Iteration 27, loss = 0.26079066\n",
            "Iteration 28, loss = 0.26095437\n",
            "Iteration 29, loss = 0.25932345\n",
            "Iteration 30, loss = 0.25805025\n",
            "Iteration 31, loss = 0.25640250\n",
            "Iteration 32, loss = 0.25470946\n",
            "Iteration 33, loss = 0.25348184\n",
            "Iteration 34, loss = 0.25296354\n",
            "Iteration 35, loss = 0.25067974\n",
            "Iteration 36, loss = 0.24933689\n",
            "Iteration 37, loss = 0.24760822\n",
            "Iteration 38, loss = 0.24690303\n",
            "Iteration 39, loss = 0.24648983\n",
            "Iteration 40, loss = 0.24521476\n",
            "Iteration 41, loss = 0.24376825\n",
            "Iteration 42, loss = 0.24394920\n",
            "Iteration 43, loss = 0.24129236\n",
            "Iteration 44, loss = 0.24189239\n",
            "Iteration 45, loss = 0.23935059\n",
            "Iteration 46, loss = 0.23830945\n",
            "Iteration 47, loss = 0.23706030\n",
            "Iteration 48, loss = 0.23667956\n",
            "Iteration 49, loss = 0.23621900\n",
            "Iteration 50, loss = 0.23391569\n",
            "Iteration 51, loss = 0.23333085\n",
            "Iteration 52, loss = 0.23290647\n",
            "Iteration 53, loss = 0.23103461\n",
            "Iteration 54, loss = 0.23048148\n",
            "Iteration 55, loss = 0.22981221\n",
            "Iteration 56, loss = 0.22863797\n",
            "Iteration 57, loss = 0.22764906\n",
            "Iteration 58, loss = 0.22650127\n",
            "Iteration 59, loss = 0.22669806\n",
            "Iteration 60, loss = 0.22519177\n",
            "Iteration 61, loss = 0.22425059\n",
            "Iteration 62, loss = 0.22368212\n",
            "Iteration 63, loss = 0.22269451\n",
            "Iteration 64, loss = 0.22183963\n",
            "Iteration 65, loss = 0.22136238\n",
            "Iteration 66, loss = 0.22082975\n",
            "Iteration 67, loss = 0.21988533\n",
            "Iteration 68, loss = 0.21798155\n",
            "Iteration 69, loss = 0.21834303\n",
            "Iteration 70, loss = 0.21812794\n",
            "Iteration 71, loss = 0.21686812\n",
            "Iteration 72, loss = 0.21640386\n",
            "Iteration 73, loss = 0.21583869\n",
            "Iteration 74, loss = 0.21520143\n",
            "Iteration 75, loss = 0.21340056\n",
            "Iteration 76, loss = 0.21336887\n",
            "Iteration 77, loss = 0.21167843\n",
            "Iteration 78, loss = 0.21230162\n",
            "Iteration 79, loss = 0.21080737\n",
            "Iteration 80, loss = 0.21026904\n",
            "Iteration 81, loss = 0.20973279\n",
            "Iteration 82, loss = 0.20944922\n",
            "Iteration 83, loss = 0.20947179\n",
            "Iteration 84, loss = 0.20707770\n",
            "Iteration 85, loss = 0.20636802\n",
            "Iteration 86, loss = 0.20665984\n",
            "Iteration 87, loss = 0.20600395\n",
            "Iteration 88, loss = 0.20461512\n",
            "Iteration 89, loss = 0.20483902\n",
            "Iteration 90, loss = 0.20450069\n",
            "Iteration 91, loss = 0.20412331\n",
            "Iteration 92, loss = 0.20380122\n",
            "Iteration 93, loss = 0.20392439\n",
            "Iteration 94, loss = 0.20114671\n",
            "Iteration 95, loss = 0.20121746\n",
            "Iteration 96, loss = 0.20130096\n",
            "Iteration 97, loss = 0.20057902\n",
            "Iteration 98, loss = 0.20079020\n",
            "Iteration 99, loss = 0.19870019\n",
            "Iteration 100, loss = 0.19890329\n",
            "Iteration 101, loss = 0.19802022\n",
            "Iteration 102, loss = 0.19820727\n",
            "Iteration 103, loss = 0.19770190\n",
            "Iteration 104, loss = 0.19667270\n",
            "Iteration 105, loss = 0.19566641\n",
            "Iteration 106, loss = 0.19857244\n",
            "Iteration 107, loss = 0.19522683\n",
            "Iteration 108, loss = 0.19411725\n",
            "Iteration 109, loss = 0.19697824\n",
            "Iteration 110, loss = 0.19355576\n",
            "Iteration 111, loss = 0.19280395\n",
            "Iteration 112, loss = 0.19247527\n",
            "Iteration 113, loss = 0.19190150\n",
            "Iteration 114, loss = 0.19275921\n",
            "Iteration 115, loss = 0.19100539\n",
            "Iteration 116, loss = 0.19155780\n",
            "Iteration 117, loss = 0.18945924\n",
            "Iteration 118, loss = 0.19016306\n",
            "Iteration 119, loss = 0.18905666\n",
            "Iteration 120, loss = 0.18856286\n",
            "Iteration 121, loss = 0.18891950\n",
            "Iteration 122, loss = 0.18790910\n",
            "Iteration 123, loss = 0.18801575\n",
            "Iteration 124, loss = 0.18654391\n",
            "Iteration 125, loss = 0.18651407\n",
            "Iteration 126, loss = 0.18575440\n",
            "Iteration 127, loss = 0.18644651\n",
            "Iteration 128, loss = 0.18544737\n",
            "Iteration 129, loss = 0.18648301\n",
            "Iteration 130, loss = 0.18459022\n",
            "Iteration 131, loss = 0.18550071\n",
            "Iteration 132, loss = 0.18486278\n",
            "Iteration 133, loss = 0.18295986\n",
            "Iteration 134, loss = 0.18269739\n",
            "Iteration 135, loss = 0.18191563\n",
            "Iteration 136, loss = 0.18083968\n",
            "Iteration 137, loss = 0.18161815\n",
            "Iteration 138, loss = 0.18179188\n",
            "Iteration 139, loss = 0.18030343\n",
            "Iteration 140, loss = 0.18121640\n",
            "Iteration 141, loss = 0.17902111\n",
            "Iteration 142, loss = 0.17940069\n",
            "Iteration 143, loss = 0.18065259\n",
            "Iteration 144, loss = 0.17963989\n",
            "Iteration 145, loss = 0.17934126\n",
            "Iteration 146, loss = 0.17995743\n",
            "Iteration 147, loss = 0.17785373\n",
            "Iteration 148, loss = 0.17877512\n",
            "Iteration 149, loss = 0.17690286\n",
            "Iteration 150, loss = 0.17714221\n",
            "Iteration 151, loss = 0.17777058\n",
            "Iteration 152, loss = 0.17633521\n",
            "Iteration 153, loss = 0.17563386\n",
            "Iteration 154, loss = 0.17466722\n",
            "Iteration 155, loss = 0.17439209\n",
            "Iteration 156, loss = 0.17392488\n",
            "Iteration 157, loss = 0.17422465\n",
            "Iteration 158, loss = 0.17438990\n",
            "Iteration 159, loss = 0.17508140\n",
            "Iteration 160, loss = 0.17539130\n",
            "Iteration 161, loss = 0.17455566\n",
            "Iteration 162, loss = 0.17358819\n",
            "Iteration 163, loss = 0.17324918\n",
            "Iteration 164, loss = 0.17254841\n",
            "Iteration 165, loss = 0.17109706\n",
            "Iteration 166, loss = 0.17165747\n",
            "Iteration 167, loss = 0.17123441\n",
            "Iteration 168, loss = 0.17088505\n",
            "Iteration 169, loss = 0.17070238\n",
            "Iteration 170, loss = 0.17021264\n",
            "Iteration 171, loss = 0.17246287\n",
            "Iteration 172, loss = 0.17255759\n",
            "Iteration 173, loss = 0.16977218\n",
            "Iteration 174, loss = 0.16928088\n",
            "Iteration 175, loss = 0.16788244\n",
            "Iteration 176, loss = 0.16874252\n",
            "Iteration 177, loss = 0.16943253\n",
            "Iteration 178, loss = 0.16780600\n",
            "Iteration 179, loss = 0.16944948\n",
            "Iteration 180, loss = 0.16737227\n",
            "Iteration 181, loss = 0.16696555\n",
            "Iteration 182, loss = 0.16686778\n",
            "Iteration 183, loss = 0.16558481\n",
            "Iteration 184, loss = 0.16767571\n",
            "Iteration 185, loss = 0.16560058\n",
            "Iteration 186, loss = 0.16496761\n",
            "Iteration 187, loss = 0.16677242\n",
            "Iteration 188, loss = 0.16374032\n",
            "Iteration 189, loss = 0.16501915\n",
            "Iteration 190, loss = 0.16747397\n",
            "Iteration 191, loss = 0.16683419\n",
            "Iteration 192, loss = 0.16557197\n",
            "Iteration 193, loss = 0.16429671\n",
            "Iteration 194, loss = 0.16521840\n",
            "Iteration 195, loss = 0.16466652\n",
            "Iteration 196, loss = 0.16439476\n",
            "Iteration 197, loss = 0.16350008\n",
            "Iteration 198, loss = 0.16232190\n",
            "Iteration 199, loss = 0.16226331\n",
            "Iteration 200, loss = 0.16384392\n",
            "Iteration 201, loss = 0.16249512\n",
            "Iteration 202, loss = 0.16262957\n",
            "Iteration 203, loss = 0.16374428\n",
            "Iteration 204, loss = 0.16126851\n",
            "Iteration 205, loss = 0.16174712\n",
            "Iteration 206, loss = 0.16161970\n",
            "Iteration 207, loss = 0.16135331\n",
            "Iteration 208, loss = 0.16028294\n",
            "Iteration 209, loss = 0.16169623\n",
            "Iteration 210, loss = 0.15993281\n",
            "Iteration 211, loss = 0.15867225\n",
            "Iteration 212, loss = 0.15954511\n",
            "Iteration 213, loss = 0.16220104\n",
            "Iteration 214, loss = 0.15788808\n",
            "Iteration 215, loss = 0.16103239\n",
            "Iteration 216, loss = 0.15767882\n",
            "Iteration 217, loss = 0.15905434\n",
            "Iteration 218, loss = 0.15982660\n",
            "Iteration 219, loss = 0.15892893\n",
            "Iteration 220, loss = 0.15955802\n",
            "Iteration 221, loss = 0.15890986\n",
            "Iteration 222, loss = 0.15668193\n",
            "Iteration 223, loss = 0.15790151\n",
            "Iteration 224, loss = 0.15613693\n",
            "Iteration 225, loss = 0.15893117\n",
            "Iteration 226, loss = 0.15615507\n",
            "Iteration 227, loss = 0.15627736\n",
            "Iteration 228, loss = 0.15700625\n",
            "Iteration 229, loss = 0.15654071\n",
            "Iteration 230, loss = 0.15717703\n",
            "Iteration 231, loss = 0.15681664\n",
            "Iteration 232, loss = 0.15566698\n",
            "Iteration 233, loss = 0.15492885\n",
            "Iteration 234, loss = 0.15518440\n",
            "Iteration 235, loss = 0.15573206\n",
            "Iteration 236, loss = 0.15596842\n",
            "Iteration 237, loss = 0.15769156\n",
            "Iteration 238, loss = 0.15527149\n",
            "Iteration 239, loss = 0.15502195\n",
            "Iteration 240, loss = 0.15386952\n",
            "Iteration 241, loss = 0.15327833\n",
            "Iteration 242, loss = 0.15196367\n",
            "Iteration 243, loss = 0.15316206\n",
            "Iteration 244, loss = 0.15349533\n",
            "Iteration 245, loss = 0.15428730\n",
            "Iteration 246, loss = 0.15372803\n",
            "Iteration 247, loss = 0.15409506\n",
            "Iteration 248, loss = 0.15326307\n",
            "Iteration 249, loss = 0.15180960\n",
            "Iteration 250, loss = 0.15417645\n",
            "Iteration 251, loss = 0.15207345\n",
            "Iteration 252, loss = 0.15157342\n",
            "Iteration 253, loss = 0.15033840\n",
            "Iteration 254, loss = 0.15100175\n",
            "Iteration 255, loss = 0.15146327\n",
            "Iteration 256, loss = 0.15109895\n",
            "Iteration 257, loss = 0.15264618\n",
            "Iteration 258, loss = 0.15083387\n",
            "Iteration 259, loss = 0.15056925\n",
            "Iteration 260, loss = 0.14960427\n",
            "Iteration 261, loss = 0.15033006\n",
            "Iteration 262, loss = 0.15104744\n",
            "Iteration 263, loss = 0.15183579\n",
            "Iteration 264, loss = 0.14856749\n",
            "Iteration 265, loss = 0.14934047\n",
            "Iteration 266, loss = 0.14840576\n",
            "Iteration 267, loss = 0.15101524\n",
            "Iteration 268, loss = 0.14864723\n",
            "Iteration 269, loss = 0.14933544\n",
            "Iteration 270, loss = 0.14735915\n",
            "Iteration 271, loss = 0.14705480\n",
            "Iteration 272, loss = 0.14857575\n",
            "Iteration 273, loss = 0.15090485\n",
            "Iteration 274, loss = 0.14837742\n",
            "Iteration 275, loss = 0.14984444\n",
            "Iteration 276, loss = 0.14855873\n",
            "Iteration 277, loss = 0.14845010\n",
            "Iteration 278, loss = 0.14727160\n",
            "Iteration 279, loss = 0.14609998\n",
            "Iteration 280, loss = 0.14748573\n",
            "Iteration 281, loss = 0.14757947\n",
            "Iteration 282, loss = 0.14630145\n",
            "Iteration 283, loss = 0.14605712\n",
            "Iteration 284, loss = 0.14616291\n",
            "Iteration 285, loss = 0.14468807\n",
            "Iteration 286, loss = 0.14555357\n",
            "Iteration 287, loss = 0.14825441\n",
            "Iteration 288, loss = 0.14693456\n",
            "Iteration 289, loss = 0.14878206\n",
            "Iteration 290, loss = 0.14695595\n",
            "Iteration 291, loss = 0.14579159\n",
            "Iteration 292, loss = 0.14725570\n",
            "Iteration 293, loss = 0.14506214\n",
            "Iteration 294, loss = 0.14538980\n",
            "Iteration 295, loss = 0.14479625\n",
            "Iteration 296, loss = 0.14467038\n",
            "Iteration 297, loss = 0.14612516\n",
            "Iteration 298, loss = 0.14452431\n",
            "Iteration 299, loss = 0.14440601\n",
            "Iteration 300, loss = 0.14484180\n",
            "Iteration 301, loss = 0.14314339\n",
            "Iteration 302, loss = 0.14556018\n",
            "Iteration 303, loss = 0.14374794\n",
            "Iteration 304, loss = 0.14618257\n",
            "Iteration 305, loss = 0.14386418\n",
            "Iteration 306, loss = 0.14337497\n",
            "Iteration 307, loss = 0.14304936\n",
            "Iteration 308, loss = 0.14205310\n",
            "Iteration 309, loss = 0.14273703\n",
            "Iteration 310, loss = 0.14391777\n",
            "Iteration 311, loss = 0.14169197\n",
            "Iteration 312, loss = 0.14426822\n",
            "Iteration 313, loss = 0.14050398\n",
            "Iteration 314, loss = 0.14384528\n",
            "Iteration 315, loss = 0.14143656\n",
            "Iteration 316, loss = 0.14150819\n",
            "Iteration 317, loss = 0.14154604\n",
            "Iteration 318, loss = 0.14115709\n",
            "Iteration 319, loss = 0.14287079\n",
            "Iteration 320, loss = 0.14111504\n",
            "Iteration 321, loss = 0.14082156\n",
            "Iteration 322, loss = 0.14010070\n",
            "Iteration 323, loss = 0.14200896\n",
            "Iteration 324, loss = 0.14214714\n",
            "Iteration 325, loss = 0.14248092\n",
            "Iteration 326, loss = 0.13979321\n",
            "Iteration 327, loss = 0.13955465\n",
            "Iteration 328, loss = 0.14464254\n",
            "Iteration 329, loss = 0.13944176\n",
            "Iteration 330, loss = 0.14088458\n",
            "Iteration 331, loss = 0.13863662\n",
            "Iteration 332, loss = 0.13940524\n",
            "Iteration 333, loss = 0.13909445\n",
            "Iteration 334, loss = 0.13970953\n",
            "Iteration 335, loss = 0.14075681\n",
            "Iteration 336, loss = 0.14042051\n",
            "Iteration 337, loss = 0.14013094\n",
            "Iteration 338, loss = 0.13958503\n",
            "Iteration 339, loss = 0.13986106\n",
            "Iteration 340, loss = 0.13995620\n",
            "Iteration 341, loss = 0.13980892\n",
            "Iteration 342, loss = 0.14004558\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8112589559877175"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJtXfSVdaknt",
        "outputId": "fabf4d95-d267-45e0-c49e-bd128e95eebb"
      },
      "source": [
        "print(classification_report(y_census_teste, previsoes))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.87      0.87      3693\n",
            "        >50K       0.61      0.63      0.62      1192\n",
            "\n",
            "    accuracy                           0.81      4885\n",
            "   macro avg       0.74      0.75      0.75      4885\n",
            "weighted avg       0.81      0.81      0.81      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}